import os
from typing import Dict, Optional
import numpy as np
from trader.factory import AlgorithmFactory
from trader.envs.factory import EnvironmentFactory
from trader.algos.base_algo import AlgorithmStrategy
from trader.envs.trading_env import TradingEnv
from trader.parallel_trainer import SubAgentEnsemble
from trader.utils.seed import SeedManager, EnvironmentSeeder
from tqdm import tqdm
from trader.utils.logging import TrainingLogger

class Trainer:
    def __init__(self, agent_name: str = 'default_agent', env: TradingEnv = None, algorithm: str = 'ddpg',
                 max_episodes: int = 100,
                 update_frequency: int = 10, model_type: str = 'mlp',
                 seed: int = 42, agent_mode: str = 'multi-agent',
                 use_attention: bool = False, num_heads: int = 4,
                 attention_type: str = 'simple', **agent_kwargs):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        :param agent_name: ä»£ç†åç¨±
        :param env: ç’°å¢ƒ
        :param algorithm: æ¼”ç®—æ³•åç¨±
        :param max_episodes: æœ€å¤§å›åˆæ•¸
        :param update_frequency: æ›´æ–°é »ç‡
        :param model_type: æ¨¡å‹é¡å‹
        :param seed: éš¨æ©Ÿç¨®å­
        :param agent_mode: ä»£ç†æ¨¡å¼ ('single-agent' æˆ– 'multi-agent')
        :param use_attention: æ˜¯å¦ä½¿ç”¨è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆåƒ… Final Agentï¼‰
        :param num_heads: æ³¨æ„åŠ›é ­æ•¸
        :param attention_type: æ³¨æ„åŠ›é¡å‹
        :param agent_kwargs: æ¼”ç®—æ³•åƒæ•¸
        """
        # â† è¨­ç½®å…¨å±€éš¨æ©Ÿç¨®å­
        SeedManager.set_seed(seed)
        self.agent_name = agent_name
        self.max_episodes = max_episodes
        self.total_timesteps = 0
        self.update_frequency = update_frequency
        self.seed = seed
        self.agent_mode = agent_mode
        self.num_of_agents = agent_kwargs.pop('num_of_subagents', 1) if agent_mode == 'multi-agent' else 1
        
        # â† æ³¨æ„åŠ›æ©Ÿåˆ¶åƒæ•¸
        self.use_attention = use_attention
        self.num_heads = num_heads
        self.attention_type = attention_type

        # â† æ·»åŠ æ¢ç´¢ç‡è¡°æ¸›åƒæ•¸
        self.initial_noise_scale = 0.3   # è¨“ç·´åˆæœŸçš„æ¢ç´¢ç‡
        self.final_noise_scale = 0.01     # è¨“ç·´å¾ŒæœŸçš„æ¢ç´¢ç‡
        
        # ç§»é™¤ max_timestepsï¼ˆå¦‚æœæœ‰å‚³å…¥çš„è©±ï¼‰
        agent_kwargs.pop('max_timesteps', None)
        
        # å¾ agent_kwargs æå–ç’°å¢ƒç›¸é—œåƒæ•¸ï¼ˆé¿å…å‚³çµ¦æ¼”ç®—æ³•ï¼‰
        env_related_keys = ['num_stocks', 'stock_symbols', 'start_date', 'end_date', 
                           'initial_balance', 'max_steps', 'transaction_cost']
        env_params = {k: agent_kwargs.pop(k) for k in env_related_keys if k in agent_kwargs}
        
        # å¾ agent_kwargs æå– kï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        k_value = agent_kwargs.pop('k', 1)
        
        # å‰µå»ºç’°å¢ƒ
        if env is None:
            env_config = {
                'num_stocks': env_params.get('num_stocks', 30),
                'stock_symbols': env_params.get('stock_symbols', []),
                'start_date': env_params.get('train_date_start', '2010-01-01'),
                'end_date': env_params.get('train_date_end', '2023-03-01'),
                'initial_balance': env_params.get('initial_balance', 100000),
                'max_steps': env_params.get('max_steps', 252),
                'k': k_value,
                'transaction_cost': env_params.get('transaction_cost', 0.001),
                'seed': seed
            }
            self.train_env = EnvironmentFactory.create_trading_env(env_config)
            self.test_env = EnvironmentFactory.create_trading_env(env_config)
        else:
            self.train_env = env
            self.test_env = self._clone_env(env)
        
        # å‰µå»ºä»£ç†ï¼ˆå‚³éæ³¨æ„åŠ›åƒæ•¸ï¼‰
        self.algo: AlgorithmStrategy = AlgorithmFactory.create(
            algorithm,
            state_dim=self.train_env.state_dim,
            action_dim=self.train_env.action_dim,
            model_type=model_type,
            k=self.train_env.k,  # ä½¿ç”¨ç’°å¢ƒçš„ k å€¼
            use_attention=use_attention,
            num_heads=num_heads,
            attention_type=attention_type,
            **agent_kwargs
        )
        
        # ç’°å¢ƒç¨®å­ç®¡ç†å™¨
        self.seeder = EnvironmentSeeder(seed)
        
        print(f"[Trainer] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - ä»£ç†åç¨±: {self.agent_name}")
        print(f"  - æ¼”ç®—æ³•: {self.algo.get_algorithm_name()}")
        print(f"  - éš¨æ©Ÿç¨®å­: {self.seed}")
        print(f"  - ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶: {self.use_attention}")
        if self.use_attention:
            print(f"  - æ³¨æ„åŠ›é¡å‹: {self.attention_type}")
            print(f"  - æ³¨æ„åŠ›é ­æ•¸: {self.num_heads}")
        print(f"  - æœ€å¤§å›åˆæ•¸: {self.max_episodes}\n")

        # åˆå§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨
        self.logger = TrainingLogger(
            agent_name=self.agent_name,
            algorithm=self.algo.get_algorithm_name(),
            agent_mode=self.agent_mode,
            log_dir='./logs',
            save_frequency=10
        )
    
    def _clone_env(self, env: TradingEnv) -> TradingEnv:
        """
        å…‹éš†ç’°å¢ƒ
        
        å¦‚æœæ˜¯ FinalAgentEnvï¼Œä¿æŒå…¶é¡å‹å’Œ SubAgentEnsemble å¼•ç”¨
        å¦‚æœæ˜¯æ™®é€š TradingEnvï¼Œé€²è¡Œæ·±åº¦å…‹éš†
        """
        # â˜…â˜…â˜… æª¢æŸ¥æ˜¯å¦ç‚º FinalAgentEnv â˜…â˜…â˜…
        if hasattr(env, 'base_env') and hasattr(env, 'ensemble'):
            # é€™æ˜¯ FinalAgentEnv
            from trader.envs.final_agent_env import FinalAgentEnv
            
            # å…‹éš†åŸºç¤ç’°å¢ƒ
            base_config = {
                'num_stocks': env.base_env.num_stocks,
                'initial_balance': env.base_env.initial_balance,
                'max_steps': env.base_env.max_steps,
                'stock_data': env.base_env.stock_data.copy(),
                'technical_indicators': env.base_env.technical_indicators.copy(),
                'fundamental_data': env.base_env.fundamental_data.copy(),
                'k': env.base_env.k,
                'transaction_cost': env.base_env.transaction_cost,
                'seed': self.seed,
                # â˜… ä¿æŒç›¸åŒçš„æ¨¡å‹é…ç½®
                'model_type': env.base_env.model_type,
                'window_size': env.base_env.window_size,

            }
            base_env_clone = TradingEnv(base_config)
            
            # ä½¿ç”¨ç›¸åŒçš„ ensemble å‰µå»ºæ–°çš„ FinalAgentEnv
            return FinalAgentEnv(base_env_clone, env.ensemble)
        else:
            # æ™®é€š TradingEnv
            config = {
                'num_stocks': env.num_stocks,
                'initial_balance': env.initial_balance,
                'max_steps': env.max_steps,
                'stock_data': env.stock_data.copy(),
                'technical_indicators': env.technical_indicators.copy(),
                'fundamental_data': env.fundamental_data.copy(),
                'k': env.k,
                'transaction_cost': env.transaction_cost,
                'seed': self.seed,
                # â˜… ä¿æŒç›¸åŒçš„æ¨¡å‹é…ç½®
                'model_type': env.model_type,
                'window_size': env.window_size,
            }
            return TradingEnv(config)
    
    def _get_noise_scale(self, episode: int) -> float:
        """
        è¨ˆç®—ç•¶å‰å›åˆçš„æ¢ç´¢ç‡ï¼ˆÎµ-decay è¡°æ¸›ï¼‰
        
        :param episode: ç•¶å‰å›åˆè™Ÿ
        :return: ç•¶å‰æ¢ç´¢ç‡
        """
        progress = episode / self.max_episodes
        # ç·šæ€§è¡°æ¸›
        noise_scale = (self.initial_noise_scale - self.final_noise_scale) * (1 - progress) + self.final_noise_scale
        return max(noise_scale, self.final_noise_scale)

    def single_agent_train(self):
        """å–®ä»£ç†è¨“ç·´å¾ªç’°"""
        print(f"[Trainer] é–‹å§‹è¨“ç·´...\n")
        
        episode = 0
        episode_rewards = []

        # é€²åº¦æ¢ï¼šåªè¿½è¹¤ Episodes
        episode_pbar = tqdm(
            total=self.max_episodes,
            desc=f"{self.agent_name} - Episodes",
            unit="ep",
            position=0,
            leave=True,
            colour='green',
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        while episode < self.max_episodes:
            self.logger.start_episode(episode)
            episode += 1

            current_noise_scale = self._get_noise_scale(episode)
            
            # â† ç‚ºæ¯å€‹ reset è¨­ç½®ç¨®å­
            reset_seed = self.seeder.get_reset_seed()
            observation, info = self.train_env.reset(seed=reset_seed)
            
            # â† è¨˜éŒ„åˆå§‹ portfolio å€¼
            initial_portfolio = info.get('portfolio_value', self.train_env.initial_balance)
            
            done = False
            truncated = False
            total_reward = 0
            step = 0
            episode_critic_loss = 0.0
            episode_actor_loss = 0.0
            update_count = 0
            
            while not done and not truncated:
                # é¸æ“‡å‹•ä½œ
                action = self.algo.select_action(observation, noise_scale=current_noise_scale)
                
                # åŸ·è¡Œå‹•ä½œ
                next_observation, reward, done, truncated, info = self.train_env.step(action)
                
                # è¨˜éŒ„æ¯æ­¥è³‡è¨Š
                self.logger.log_step(step, action, reward, info['portfolio_value'], info['balance'])

                # å­˜å„²ç¶“é©—
                if hasattr(self.algo, 'store_experience'):
                    self.algo.store_experience(observation, action, reward, next_observation, done)
                
                # å®šæœŸæ›´æ–°æ¨¡å‹
                if step % self.update_frequency == 0:
                    losses = self.algo.update_model()
                    episode_critic_loss += losses.get('critic_loss', 0.0)
                    episode_actor_loss += losses.get('actor_loss', 0.0)
                    update_count += 1
                
                observation = next_observation
                total_reward += reward
                step += 1
                self.total_timesteps += 1
            
            # â† è¨ˆç®—å¹³å‡ lossï¼ˆç§»åˆ° while è¿´åœˆå¤–ï¼‰
            avg_critic_loss = episode_critic_loss / max(update_count, 1)
            avg_actor_loss = episode_actor_loss / max(update_count, 1)
            
            # â† Episode çµæŸå¾Œè¨˜éŒ„ï¼ˆç§»åˆ° while è¿´åœˆå¤–ï¼‰
            self.logger.end_episode(
                episode_reward=total_reward,
                episode_length=step,
                actor_loss=avg_actor_loss,
                critic_loss=avg_critic_loss,
                noise_scale=current_noise_scale,
                initial_portfolio=initial_portfolio,
                final_portfolio=info['portfolio_value']
            )
            
            episode_rewards.append(total_reward)
            
            # æ›´æ–° episode é€²åº¦æ¢
            episode_pbar.update(1)
            episode_pbar.set_postfix({
                'reward': f'{total_reward:.2f}',
                'avg_reward': f'{np.mean(episode_rewards[-10:]):.2f}',
                'c_loss': f'{avg_critic_loss:.4f}',
                'a_loss': f'{avg_actor_loss:.4f}',
                'steps': step
            })

        # è¨“ç·´çµæŸ
        self.logger.finalize()

        # é—œé–‰é€²åº¦æ¢
        episode_pbar.close()
        
        print(f"\n{'='*70}")
        print(f"[Trainer] âœ… Agent '{self.agent_name}' è¨“ç·´å®Œæˆï¼")
        print(f"  - Total episodes: {episode}")
        print(f"  - Total timesteps: {self.total_timesteps}")
        print(f"  - Average reward: {np.mean(episode_rewards):.4f}")
        print(f"  - Best reward: {np.max(episode_rewards):.4f}")
        print(f"  - Last 10 avg reward: {np.mean(episode_rewards[-10:]):.4f}\n")
        
        return {
            'episodes': episode,
            'total_timesteps': self.total_timesteps,
            'episode_rewards': episode_rewards
        }

    def multi_agent_train(self):
        """å¤šä»£ç†è¨“ç·´"""
        for agent_id in range(self.num_of_agents):
            print(f"\n{'='*30} è¨“ç·´ä»£ç† {agent_id+1}/{self.num_of_agents} {'='*30}\n")
            self.single_agent_train()

    def train(self):
        """è¨“ç·´å¾ªç’°"""
        if self.agent_mode == 'single-agent':
            return self.single_agent_train()
        elif self.agent_mode == 'multi-agent':
            self.multi_agent_train()
        else:
            raise ValueError(f"æœªçŸ¥çš„ä»£ç†æ¨¡å¼: {self.agent_mode}")
    
    def evaluate(self, deterministic_seed: bool = True):
        """
        è©•ä¼°æ¨¡å‹æ€§èƒ½ - èµ°å®Œæ•´å€‹æ¸¬è©¦æœŸé–“çš„æ‰€æœ‰ timesteps
        
        :param deterministic_seed: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç¨®å­
        """
        print(f"\n{'='*70}")
        print(f"[Trainer] ğŸ§ª é–‹å§‹å›æ¸¬ Agent: {self.agent_name}")
        print(f"  - Deterministic seed: {deterministic_seed}\n")
        
        if deterministic_seed:
            eval_seed = self.seed
        else:
            eval_seed = self.seeder.get_reset_seed()
        
        observation, info = self.test_env.reset(seed=eval_seed)
        done = False
        truncated = False
        total_reward = 0
        step = 0
        
        # è¨˜éŒ„æ¯æ—¥æ•¸æ“š
        daily_rewards = []
        daily_portfolio_values = []
        daily_actions = []
        
        # â˜…â˜…â˜… è¨ˆç®—ç¸½äº¤æ˜“æ—¥æ•¸ â˜…â˜…â˜…
        # å¾ç’°å¢ƒçš„è‚¡ç¥¨æ•¸æ“šç²å–äº¤æ˜“æ—¥æ•¸é‡
        if hasattr(self.test_env, 'base_env'):
            # FinalAgentEnv çš„æƒ…æ³
            total_trading_days = self.test_env.base_env.stock_data.shape[0]
        else:
            # æ™®é€š TradingEnv çš„æƒ…æ³
            total_trading_days = self.test_env.stock_data.shape[0]
        
        print(f"  - Total trading days: {total_trading_days}\n")
        
        # â˜…â˜…â˜… ä½¿ç”¨è¨ˆç®—å‡ºçš„äº¤æ˜“æ—¥æ•¸ä½œç‚ºé€²åº¦æ¢ä¸Šé™ â˜…â˜…â˜…
        eval_pbar = tqdm(
            total=total_trading_days,
            desc=f"ğŸ§ª {self.agent_name} - Backtesting",
            unit="day",
            colour='yellow',
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        initial_portfolio = info.get('portfolio_value', self.test_env.initial_balance)
        
        while not done and not truncated:
            # ä½¿ç”¨ deterministic action selectionï¼ˆç„¡å™ªéŸ³ï¼‰
            action = self.algo.select_action_deterministic(observation)
            next_observation, reward, done, truncated, info = self.test_env.step(action)
            
            total_reward += reward
            step += 1
            
            # è¨˜éŒ„æ¯æ—¥æ•¸æ“š
            daily_rewards.append(reward)
            daily_portfolio_values.append(info.get('portfolio_value', 0))
            daily_actions.append(action.copy())
            
            observation = next_observation
            
            # æ›´æ–°é€²åº¦æ¢
            eval_pbar.update(1)
            eval_pbar.set_postfix({
                'portfolio': f"${info.get('portfolio_value', 0):,.0f}",
                'reward': f'{reward:.4f}',
                'total': f'{total_reward:.2f}'
            })
        
        eval_pbar.close()
        
        # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
        final_portfolio = info.get('portfolio_value', 0)
        # â† ä¿è­·ï¼šé¿å…é™¤ä»¥é›¶
        total_return = 0.0 if initial_portfolio == 0 else (final_portfolio - initial_portfolio) / initial_portfolio * 100
        
        # è¨ˆç®—æ¯æ—¥å ±é…¬ç‡ï¼ˆæ·»åŠ ä¿è­·æ©Ÿåˆ¶ï¼‰
        daily_returns = []
        if len(daily_portfolio_values) > 1:
            for i in range(1, len(daily_portfolio_values)):
                prev_value = daily_portfolio_values[i - 1]
                curr_value = daily_portfolio_values[i]
                # â† é¿å…é™¤ä»¥é›¶å’Œç„¡æ•ˆå€¼
                if prev_value > 0:
                    daily_ret = (curr_value - prev_value) / prev_value
                    if np.isfinite(daily_ret):
                        daily_returns.append(daily_ret)
        
        daily_returns = np.array(daily_returns) if daily_returns else np.array([])
        
        # è¨ˆç®— Sharpe Ratio (å‡è¨­ç„¡é¢¨éšªåˆ©ç‡ç‚º 0ï¼Œå¹´åŒ–)
        if len(daily_returns) > 0 and np.std(daily_returns) > 0:
            sharpe_ratio = np.mean(daily_returns) / np.std(daily_returns) * np.sqrt(252)
        else:
            sharpe_ratio = 0.0
        
        # è¨ˆç®—æœ€å¤§å›æ’¤ (Maximum Drawdown)ï¼ˆæ·»åŠ ä¿è­·æ©Ÿåˆ¶ï¼‰
        max_drawdown = 0.0
        if len(daily_portfolio_values) > 0:
            peak = np.maximum.accumulate(daily_portfolio_values)
            # â† é¿å…é™¤ä»¥é›¶
            drawdown = np.zeros_like(peak, dtype=float)
            for i, p in enumerate(peak):
                if p > 0:
                    drawdown[i] = (p - daily_portfolio_values[i]) / p
            max_drawdown = np.max(drawdown) * 100 if len(drawdown) > 0 else 0.0
        
        print(f"\n{'='*70}")
        print(f"[Trainer] ğŸ“Š å›æ¸¬çµæœ - {self.agent_name}")
        print(f"{'='*70}")
        print(f"  ğŸ“… å›æ¸¬æœŸé–“: {step} å€‹äº¤æ˜“æ—¥ (ä½”ç¸½äº¤æ˜“æ—¥æ•¸ {step}/{total_trading_days})")
        print(f"  ğŸ’° åˆå§‹è³‡é‡‘: ${initial_portfolio:,.2f}")
        print(f"  ğŸ’µ æœ€çµ‚è³‡é‡‘: ${final_portfolio:,.2f}")
        print(f"  ğŸ“ˆ ç¸½å ±é…¬ç‡: {total_return:.2f}%")
        print(f"  ğŸ“‰ æœ€å¤§å›æ’¤: {max_drawdown:.2f}%")
        print(f"  ğŸ“Š Sharpe Ratio: {sharpe_ratio:.4f}")
        print(f"  ğŸ¯ ç´¯ç©çå‹µ: {total_reward:.4f}")
        print(f"{'='*70}\n")
        
        return {
            'agent_name': self.agent_name,
            'total_steps': step,
            'total_trading_days': total_trading_days,
            'initial_portfolio': initial_portfolio,
            'final_portfolio': final_portfolio,
            'total_return': total_return,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': sharpe_ratio,
            'total_reward': total_reward,
            'daily_rewards': daily_rewards,
            'daily_portfolio_values': daily_portfolio_values,
            'daily_actions': daily_actions
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        self.algo.save_model(save_path)
        print(f"[Trainer] æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}")
    
    def load_model(self, path: str):
        """è¼‰å…¥æ¨¡å‹"""
        self.algo.load_model(path)
        print(f"[Trainer] æ¨¡å‹å·²å¾ {path} åŠ è¼‰")


class HierarchicalTrainer:
    """
    åˆ†å±¤è¨“ç·´å™¨
    
    1. å¹³è¡Œè¨“ç·´ Sub-Agentsï¼ˆä¸ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
    2. è¼‰å…¥ Sub-Agent æ¨¡å‹
    3. è¨“ç·´ Final Agentï¼ˆä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
    """
    
    def __init__(self, config: Dict, seed: int = 42):
        """
        åˆå§‹åŒ–åˆ†å±¤è¨“ç·´å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
            seed: éš¨æ©Ÿç¨®å­
        """
        from trader.parallel_trainer import ParallelSubAgentTrainer, SubAgentEnsemble
        from trader.envs.final_agent_env import FinalAgentEnv
        
        self.config = config
        self.seed = seed
        
        # Sub-Agent è¨“ç·´å™¨
        self.parallel_trainer = ParallelSubAgentTrainer(config, seed)
        
        # Sub-Agent é›†æˆå™¨ï¼ˆè¨“ç·´å¾Œåˆå§‹åŒ–ï¼‰
        self.ensemble: Optional[SubAgentEnsemble] = None
        
        # Final Agent ç’°å¢ƒå’Œè¨“ç·´å™¨ï¼ˆè¨“ç·´ Sub-Agent å¾Œåˆå§‹åŒ–ï¼‰
        self.final_env: Optional[FinalAgentEnv] = None
        self.final_trainer: Optional[Trainer] = None
        
        self.sub_agent_results = {}
    
    def train(self, num_workers: int = None):
        """
        å®Œæ•´è¨“ç·´æµç¨‹
        
        1. å¹³è¡Œè¨“ç·´ Sub-Agentsï¼ˆç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
        2. è¼‰å…¥ Sub-Agent æ¨¡å‹ä¸¦å»ºç«‹é›†æˆå™¨
        3. å‰µå»º Final Agent ç’°å¢ƒ
        4. è¨“ç·´ Final Agentï¼ˆä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
        """
        from trader.parallel_trainer import ParallelSubAgentTrainer, SubAgentEnsemble
        from trader.envs.final_agent_env import FinalAgentEnv
        from trader.envs.factory import EnvironmentFactory
        
        print(f"\n{'='*70}")
        print(f"[HierarchicalTrainer] ğŸš€ é–‹å§‹åˆ†å±¤è¨“ç·´")
        print(f"{'='*70}\n")
        
        # ========== éšæ®µ 1: å¹³è¡Œè¨“ç·´ Sub-Agents ==========
        print(f"\n{'='*70}")
        print(f"[éšæ®µ 1/3] å¹³è¡Œè¨“ç·´ Sub-Agentsï¼ˆç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰")
        print(f"{'='*70}\n")
        
        self.sub_agent_results = self.parallel_trainer.train_sub_agents_parallel(num_workers)
        
        # ========== éšæ®µ 2: è¼‰å…¥ Sub-Agent æ¨¡å‹ ==========
        print(f"\n{'='*70}")
        print(f"[éšæ®µ 2/3] è¼‰å…¥ Sub-Agent æ¨¡å‹ä¸¦å»ºç«‹é›†æˆå™¨")
        print(f"{'='*70}\n")
        
        self.ensemble = self._create_ensemble()
        
        # ========== éšæ®µ 3: è¨“ç·´ Final Agent ==========
        print(f"\n{'='*70}")
        print(f"[éšæ®µ 3/3] è¨“ç·´ Final Agentï¼ˆä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰")
        print(f"{'='*70}\n")
        
        self._train_final_agent()
        
        print(f"\n{'='*70}")
        print(f"[HierarchicalTrainer] âœ… åˆ†å±¤è¨“ç·´å®Œæˆï¼")
        print(f"{'='*70}\n")
    
    def _create_ensemble(self) -> SubAgentEnsemble:
        """å»ºç«‹ Sub-Agent é›†æˆå™¨"""
        from trader.parallel_trainer import SubAgentEnsemble
        from trader.envs.factory import EnvironmentFactory
        
        data_cfg = self.config['data']
        env_cfg = self.config['env']
        hyper_cfg = self.config['hyperparameters']
        
        # å‰µå»ºä¸€å€‹è‡¨æ™‚ç’°å¢ƒä¾†ç²å– state_dim å’Œ action_dim
        temp_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(data_cfg['ticker_list']),
            'stock_symbols': data_cfg['ticker_list'],
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': data_cfg['date_start'],
            'end_date': data_cfg['date_end'],
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': self.seed,
        })
        
        # æº–å‚™æ¨¡å‹è¼‰å…¥è³‡è¨Š
        model_paths = {}
        sub_agents_cfg = self.config['agent_mode'].get('sub_agents', [])
        
        for i, sub_agent in enumerate(sub_agents_cfg):
            agent_name = sub_agent.get('name', f'Sub-Agent-{i}')
            
            if agent_name in self.sub_agent_results:
                result = self.sub_agent_results[agent_name]
                if result['status'] == 'success':
                    model_paths[agent_name] = {
                        'path': result['model_path'],
                        'algorithm': sub_agent.get('algorithm', 'a2c'),
                        'model_type': sub_agent.get('model_type', 'mlp'),
                        'state_dim': temp_env.state_dim,
                        'action_dim': temp_env.action_dim,
                        'hidden_dim': int(hyper_cfg['hidden_dim']),
                    }
        
        return SubAgentEnsemble(model_paths)
    
    def _train_final_agent(self):
        """è¨“ç·´ Final Agentï¼ˆä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰"""
        from trader.envs.final_agent_env import FinalAgentEnv
        from trader.envs.factory import EnvironmentFactory
        
        data_cfg = self.config['data']
        env_cfg = self.config['env']
        train_cfg = self.config['training']
        hyper_cfg = self.config['hyperparameters']
        final_agent_cfg = self.config['agent_mode'].get('final_agent', {})
        
        # â˜… æå–æ¨¡å‹é…ç½®
        model_type = final_agent_cfg.get('model_type', 'mlp')
    

        # â˜…â˜…â˜… æå–è¨“ç·´å’Œæ¸¬è©¦æ—¥æœŸ â˜…â˜…â˜…
        train_start = data_cfg.get('train_date_start', data_cfg.get('date_start', '2010-01-01'))
        train_end = data_cfg.get('train_date_end', data_cfg.get('date_end', '2021-10-01'))
        test_start = data_cfg.get('test_date_start', data_cfg.get('date_start', '2021-10-01'))
        test_end = data_cfg.get('test_date_end', data_cfg.get('date_end', '2023-03-01'))
        
        print(f"\n[HierarchicalTrainer] ğŸ“… æ—¥æœŸç¯„åœ:")
        print(f"  - è¨“ç·´æœŸé–“: {train_start} è‡³ {train_end}")
        print(f"  - æ¸¬è©¦æœŸé–“: {test_start} è‡³ {test_end}\n")
        
        # â˜…â˜…â˜… å‰µå»ºè¨“ç·´ç’°å¢ƒï¼ˆä½¿ç”¨è¨“ç·´æ—¥æœŸï¼‰ â˜…â˜…â˜…
        train_base_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(data_cfg['ticker_list']),
            'stock_symbols': data_cfg['ticker_list'],
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': train_start,
            'end_date': train_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': self.seed + 100,
            'agent_type': final_agent_cfg.get('agent_type', 'final'),
            # â˜… æ¨¡å‹é…ç½®
            'model_type': model_type,
            'window_size': env_cfg.get('window_size', 10),
        })
        
        # åŒ…è£ç‚º Final Agent è¨“ç·´ç’°å¢ƒ
        train_final_env = FinalAgentEnv(train_base_env, self.ensemble)
        
        # â˜…â˜…â˜… å‰µå»ºæ¸¬è©¦ç’°å¢ƒï¼ˆä½¿ç”¨æ¸¬è©¦æ—¥æœŸï¼‰ â˜…â˜…â˜…
        test_base_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(data_cfg['ticker_list']),
            'stock_symbols': data_cfg['ticker_list'],
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': test_start,
            'end_date': test_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': self.seed + 101,
            'agent_type': final_agent_cfg.get('agent_type', 'final'),
            # â˜… æ¨¡å‹é…ç½®
            'model_type': model_type,
            'window_size': env_cfg.get('window_size', 10),
        })
        
        # åŒ…è£ç‚º Final Agent æ¸¬è©¦ç’°å¢ƒ
        test_final_env = FinalAgentEnv(test_base_env, self.ensemble)
        
        # â˜…â˜…â˜… é—œéµä¿®æ­£ï¼šä½¿ç”¨ FinalAgentEnv çš„ state_dim â˜…â˜…â˜…
        # FinalAgentEnv.state_dim = base_state_dim + q_values_dim
        print(f"[HierarchicalTrainer] Final Agent ç’°å¢ƒè¨­ç½®:")
        print(f"  - Base state dim: {train_base_env.state_dim}")
        print(f"  - Q-values dim: {self.ensemble.get_q_values_dim()}")
        print(f"  - Total state dim: {train_final_env.state_dim}")
        print(f"  - Action dim: {train_final_env.action_dim}")
        
        # å¾é…ç½®æå– Final Agent çš„æ³¨æ„åŠ›åƒæ•¸
        use_attention = final_agent_cfg.get('use_attention', False)
        num_heads = final_agent_cfg.get('num_heads', 4)
        attention_type = final_agent_cfg.get('attention_type', 'simple')
        
        # å‰µå»º Final Agent è¨“ç·´å™¨ï¼ˆä½¿ç”¨æ­£ç¢ºçš„ state_dim å’Œæ³¨æ„åŠ›åƒæ•¸ï¼‰
        self.final_trainer = Trainer(
            agent_name=final_agent_cfg.get('name', 'Final_Agent'),
            env=train_final_env,  # â˜…â˜…â˜… ä½¿ç”¨è¨“ç·´ç’°å¢ƒ â˜…â˜…â˜…
            algorithm=final_agent_cfg.get('algorithm', 'ddpg'),
            max_episodes=train_cfg['max_episodes'],
            update_frequency=train_cfg['update_frequency'],
            model_type=final_agent_cfg.get('model_type', 'mlp'),
            seed=self.seed + 100,
            agent_mode='single-agent',
            use_attention=use_attention,
            num_heads=num_heads,
            attention_type=attention_type,
            actor_lr=float(hyper_cfg['actor_lr']),
            critic_lr=float(hyper_cfg['critic_lr']),
            gamma=float(hyper_cfg['gamma']),
            hidden_dim=int(hyper_cfg['hidden_dim']),
            batch_size=int(hyper_cfg['batch_size']),
        )
        
        # â˜…â˜…â˜… æ‰‹å‹•è¨­ç½®æ¸¬è©¦ç’°å¢ƒ â˜…â˜…â˜…
        self.final_trainer.test_env = test_final_env
        
        # è¨“ç·´
        self.final_trainer.train()
        
        # å„²å­˜æ¨¡å‹
        os.makedirs('./models', exist_ok=True)
        final_model_path = f"./models/{final_agent_cfg.get('name', 'Final_Agent')}_agent.pth"
        self.final_trainer.save_model(final_model_path)

    def initialize_for_evaluation(self):
        """
        åˆå§‹åŒ–è©•ä¼°ç’°å¢ƒ
        
        åœ¨è©•ä¼°æ¨¡å¼ä¸‹ï¼Œå¾å·²ä¿å­˜çš„æ¨¡å‹åŠ è¼‰ Sub-Agents å’Œ Final Agent
        """
        from trader.parallel_trainer import SubAgentEnsemble
        from trader.envs.final_agent_env import FinalAgentEnv
        from trader.envs.factory import EnvironmentFactory
        
        data_cfg = self.config['data']
        env_cfg = self.config['env']
        train_cfg = self.config['training']
        hyper_cfg = self.config['hyperparameters']
        final_agent_cfg = self.config['agent_mode'].get('final_agent', {})
        
        # â˜…â˜…â˜… æå–æ¸¬è©¦æ—¥æœŸ â˜…â˜…â˜…
        test_start = data_cfg.get('test_date_start', data_cfg.get('date_start', '2021-10-01'))
        test_end = data_cfg.get('test_date_end', data_cfg.get('date_end', '2023-03-01'))
        
        print(f"\n[HierarchicalTrainer] åˆå§‹åŒ–è©•ä¼°ç’°å¢ƒ...")
        print(f"  ğŸ“… è©•ä¼°æœŸé–“: {test_start} è‡³ {test_end}\n")
        
        # ========== ç¬¬ä¸€æ­¥ï¼šå»ºç«‹ Sub-Agent é›†æˆå™¨ ==========
        if self.ensemble is None:
            print(f"\n  [éšæ®µ 1/3] è¼‰å…¥ Sub-Agent æ¨¡å‹...")
            sub_agent_model_dir = './models/sub_agents'
            
            if not os.path.exists(sub_agent_model_dir):
                raise FileNotFoundError(f"Sub-Agent æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {sub_agent_model_dir}")
            
            model_paths = {}
            sub_agents_cfg = self.config['agent_mode'].get('sub_agents', [])
            
            # â˜…â˜…â˜… ä½¿ç”¨æ¸¬è©¦æ—¥æœŸå‰µå»ºè‡¨æ™‚ç’°å¢ƒ â˜…â˜…â˜…
            temp_env = EnvironmentFactory.create_trading_env({
                'num_stocks': len(data_cfg['ticker_list']),
                'stock_symbols': data_cfg['ticker_list'],
                'initial_balance': env_cfg['initial_balance'],
                'max_steps': env_cfg['max_steps'],
                'start_date': test_start,
                'end_date': test_end,
                'transaction_cost': env_cfg['transaction_cost'],
                'seed': self.seed,
            })
            
            print(f"    åŸºç¤ç’°å¢ƒç¶­åº¦: state_dim={temp_env.state_dim}, action_dim={temp_env.action_dim}")
            
            for i, sub_agent in enumerate(sub_agents_cfg):
                agent_name = sub_agent.get('name', f'Sub-Agent-{i}')
                model_file = os.path.join(sub_agent_model_dir, f"{agent_name}_agent.pth")
                
                if os.path.exists(model_file):
                    model_paths[agent_name] = {
                        'path': model_file,
                        'algorithm': sub_agent.get('algorithm', 'a2c'),
                        'model_type': sub_agent.get('model_type', 'mlp'),
                        'state_dim': temp_env.state_dim,
                        'action_dim': temp_env.action_dim,
                        'hidden_dim': int(hyper_cfg.get('hidden_dim', 256)),
                    }
                    print(f"    âœ“ {agent_name}: {model_file}")
                else:
                    print(f"    âœ— {agent_name}: æœªæ‰¾åˆ° {model_file}")
            
            if not model_paths:
                raise ValueError(f"æœªèƒ½åŠ è¼‰ä»»ä½• Sub-Agent æ¨¡å‹ï¼Œè«‹ç¢ºèª {sub_agent_model_dir} ç›®éŒ„ä¸­æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶")
            
            print(f"\n    å»ºç«‹ Sub-Agent é›†æˆå™¨...")
            self.ensemble = SubAgentEnsemble(model_paths)
            print(f"    âœ“ Sub-Agent é›†æˆå™¨å»ºç«‹å®Œæˆ ({len(model_paths)} å€‹ä»£ç†)")
        
        # ========== ç¬¬äºŒæ­¥ï¼šé©—è­‰å’Œå»ºç«‹ Final Agent ç’°å¢ƒ ==========
        if self.final_env is None:
            print(f"\n  [éšæ®µ 2/3] å‰µå»º Final Agent ç’°å¢ƒ...")
            
            # â˜…â˜…â˜… ä½¿ç”¨æ¸¬è©¦æ—¥æœŸå‰µå»ºè©•ä¼°ç’°å¢ƒ â˜…â˜…â˜…
            base_env = EnvironmentFactory.create_trading_env({
                'num_stocks': len(data_cfg['ticker_list']),
                'stock_symbols': data_cfg['ticker_list'],
                'initial_balance': env_cfg['initial_balance'],
                'max_steps': env_cfg['max_steps'],
                'start_date': test_start,
                'end_date': test_end,
                'transaction_cost': env_cfg['transaction_cost'],
                'seed': self.seed + 100,
                'agent_type': final_agent_cfg.get('agent_type', 'final'),
            })
            
            # åŒ…è£ç‚º Final Agent ç’°å¢ƒ
            self.final_env = FinalAgentEnv(base_env, self.ensemble)
            
            print(f"    âœ“ Final Agent ç’°å¢ƒå·²å»ºç«‹")
            print(f"      - Base state dim: {self.final_env.base_state_dim}")
            print(f"      - Q-values dim: {self.final_env.q_values_dim}")
            print(f"      - Total state dim: {self.final_env.state_dim}")
            
            # â˜…â˜…â˜… é©—è­‰ç‹€æ…‹ç¶­åº¦ â˜…â˜…â˜…
            try:
                test_obs, _ = self.final_env.reset(seed=self.seed)
                actual_state_dim = len(test_obs)
                print(f"    âœ“ æ¸¬è©¦ reset æˆåŠŸï¼Œå¯¦éš› state dim: {actual_state_dim}")
                
                if actual_state_dim != self.final_env.state_dim:
                    print(f"    âš ï¸ è­¦å‘Šï¼šç‹€æ…‹ç¶­åº¦ä¸ä¸€è‡´ (é æœŸ: {self.final_env.state_dim}, å¯¦éš›: {actual_state_dim})")
            except Exception as e:
                print(f"    âš ï¸ è­¦å‘Šï¼šæ¸¬è©¦ reset å¤±æ•—: {e}")
        
        # ========== ç¬¬ä¸‰æ­¥ï¼šå»ºç«‹å’ŒåŠ è¼‰ Final Agent ==========
        if self.final_trainer is None:
            print(f"\n  [éšæ®µ 3/3] å‰µå»ºä¸¦åŠ è¼‰ Final Agent è¨“ç·´å™¨...")
            
            use_attention = final_agent_cfg.get('use_attention', False)
            num_heads = final_agent_cfg.get('num_heads', 4)
            attention_type = final_agent_cfg.get('attention_type', 'simple')
            
            # â˜…â˜…â˜… ä½¿ç”¨è©•ä¼°ç’°å¢ƒçš„å¯¦éš› state_dim â˜…â˜…â˜…
            actual_state_dim = self.final_env.state_dim
            print(f"    ä½¿ç”¨è©•ä¼°ç’°å¢ƒçš„ state_dim: {actual_state_dim}")
            
            # å‰µå»ºè¨“ç·´å™¨
            self.final_trainer = Trainer(
                agent_name=final_agent_cfg.get('name', 'Final_Agent'),
                env=self.final_env,
                algorithm=final_agent_cfg.get('algorithm', 'ddpg'),
                max_episodes=train_cfg['max_episodes'],
                update_frequency=train_cfg['update_frequency'],
                model_type=final_agent_cfg.get('model_type', 'mlp'),
                seed=self.seed + 100,
                agent_mode='single-agent',
                use_attention=use_attention,
                num_heads=num_heads,
                attention_type=attention_type,
                actor_lr=float(hyper_cfg['actor_lr']),
                critic_lr=float(hyper_cfg['critic_lr']),
                gamma=float(hyper_cfg['gamma']),
                hidden_dim=int(hyper_cfg.get('hidden_dim', 256)),
                batch_size=int(hyper_cfg.get('batch_size', 64)),
            )
            
            # â˜…â˜…â˜… é©—è­‰è¼‰å…¥çš„æ¨¡å‹èˆ‡ç•¶å‰ç’°å¢ƒç¶­åº¦æ˜¯å¦åŒ¹é… â˜…â˜…â˜…
            final_agent_name = final_agent_cfg.get('name', 'Final_Agent')
            model_path = f"./models/{final_agent_name}_agent.pth"
            
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Final Agent æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
            
            # åœ¨è¼‰å…¥å‰æª¢æŸ¥æ¨¡å‹çš„ç‹€æ…‹ç¶­åº¦
            print(f"    æª¢æŸ¥æ¨¡å‹å…¼å®¹æ€§...")
            try:
                # å˜—è©¦è¼‰å…¥ä¸¦æª¢æŸ¥ç¬¬ä¸€å€‹å±¤çš„è¼¸å…¥ç¶­åº¦
                import torch
                checkpoint = torch.load(model_path, map_location='cpu')
                
                # æª¢æŸ¥æ¨¡å‹ä¸­çš„ç¬¬ä¸€å€‹ç·šæ€§å±¤
                if hasattr(self.final_trainer.algo, 'actor') and self.final_trainer.algo.actor is not None:
                    actor_model = self.final_trainer.algo.actor
                    # ç²å–ç¬¬ä¸€å€‹å±¤çš„æ¬Šé‡
                    for module in actor_model.modules():
                        if isinstance(module, torch.nn.Linear):
                            model_input_dim = module.weight.shape[1]
                            print(f"      æ¨¡å‹æœŸæœ›çš„è¼¸å…¥ç¶­åº¦: {model_input_dim}")
                            print(f"      ç’°å¢ƒæä¾›çš„ç‹€æ…‹ç¶­åº¦: {actual_state_dim}")
                            if model_input_dim != actual_state_dim:
                                print(f"      âš ï¸ ç¶­åº¦ä¸åŒ¹é…ï¼é€™å¯èƒ½å°è‡´è©•ä¼°å¤±æ•—")
                            break
            except Exception as e:
                print(f"    âš ï¸ ç„¡æ³•é©—è­‰æ¨¡å‹ç¶­åº¦: {e}")
            
            # è¼‰å…¥æ¨¡å‹
            self.final_trainer.load_model(model_path)
            print(f"    âœ“ Final Agent æ¨¡å‹å·²è¼‰å…¥: {model_path}")
        
        print(f"\nâœ… è©•ä¼°ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ\n")

    def evaluate(self, deterministic_seed: bool = True):
        """
        è©•ä¼° Final Agent
        
        å¦‚æœ final_trainer é‚„æœªåˆå§‹åŒ–ï¼Œè‡ªå‹•åˆå§‹åŒ–è©•ä¼°ç’°å¢ƒï¼ˆåŒ…å«æ‰€æœ‰ Sub-Agentsï¼‰
        """
        # ç¢ºä¿è©•ä¼°ç’°å¢ƒå·²åˆå§‹åŒ–ï¼ˆåŒ…æ‹¬ Sub-Agents å’Œ Final Agentï¼‰
        if self.final_trainer is None:
            try:
                self.initialize_for_evaluation()
            except Exception as e:
                print(f"\nâŒ è©•ä¼°ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—: {e}")
                raise
        
        if self.final_trainer is None:
            raise ValueError("ç„¡æ³•åˆå§‹åŒ–è©•ä¼°ç’°å¢ƒï¼Œè«‹ç¢ºèªæ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        
        return self.final_trainer.evaluate(deterministic_seed=deterministic_seed)
    
    def save_all_models(self, base_path: str = './models'):
        """å„²å­˜æ‰€æœ‰æ¨¡å‹"""
        os.makedirs(base_path, exist_ok=True)
        
        # Sub-Agent æ¨¡å‹å·²åœ¨è¨“ç·´æ™‚å„²å­˜
        print(f"[HierarchicalTrainer] Sub-Agent æ¨¡å‹ä½ç½®:")
        for name, result in self.sub_agent_results.items():
            if result['status'] == 'success':
                print(f"  - {name}: {result['model_path']}")
        
        # å„²å­˜ Final Agent
        if self.final_trainer is not None:
            final_path = os.path.join(base_path, 'Final_Agent.pth')
            self.final_trainer.save_model(final_path)
            print(f"  - Final Agent: {final_path}")