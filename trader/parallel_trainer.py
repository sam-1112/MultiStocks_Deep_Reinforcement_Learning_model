"""
å¹³è¡ŒåŒ– Sub-Agent è¨“ç·´å™¨
"""

import os
import numpy as np
import torch
import multiprocessing as mp
from multiprocessing import Process, Queue, Manager
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm

from trader.factory import AlgorithmFactory
from trader.envs.factory import EnvironmentFactory
from trader.envs.trading_env import TradingEnv
from trader.algos.base_algo import AlgorithmStrategy
from trader.utils.seed import SeedManager, EnvironmentSeeder
from trader.utils.logging import TrainingLogger


def train_single_sub_agent(
    agent_config: Dict,
    result_queue: Queue,
    progress_queue: Queue = None
):
    """
    è¨“ç·´å–®å€‹ Sub-Agentï¼ˆåœ¨ç¨ç«‹é€²ç¨‹ä¸­åŸ·è¡Œï¼‰
    
    Args:
        agent_config: Sub-Agent é…ç½®
        result_queue: ç”¨æ–¼å›žå‚³è¨“ç·´çµæžœçš„ Queue
        progress_queue: ç”¨æ–¼å›žå ±é€²åº¦çš„ Queue
    """
    try:
        agent_name = agent_config['name']
        agent_id = agent_config['agent_id']
        seed = agent_config['seed']
        device = agent_config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        
        # è¨­ç½®ç¨®å­
        SeedManager.set_seed(seed)
        
        # å‰µå»ºç’°å¢ƒ
        env = EnvironmentFactory.create_trading_env(agent_config['env_config'])
        
        # å‰µå»ºæ¼”ç®—æ³•ï¼ˆæŒ‡å®šè¨­å‚™ï¼‰
        algo_kwargs = agent_config.get('algo_kwargs', {})
        algo_kwargs['device'] = device
        
        algo = AlgorithmFactory.create(
            agent_config['algorithm'],
            state_dim=env.state_dim,
            action_dim=env.action_dim,
            model_type=agent_config['model_type'],
            **algo_kwargs
        )
        
        # ç’°å¢ƒç¨®å­ç®¡ç†å™¨
        seeder = EnvironmentSeeder(seed)
        
        # è¨“ç·´åƒæ•¸
        max_episodes = agent_config['max_episodes']
        update_frequency = agent_config['update_frequency']
        initial_noise_scale = agent_config.get('initial_noise_scale', 0.3)
        final_noise_scale = agent_config.get('final_noise_scale', 0.01)
        
        episode_rewards = []
        
        for episode in range(max_episodes):
            # è¨ˆç®—æŽ¢ç´¢çŽ‡
            progress = episode / max_episodes
            noise_scale = (initial_noise_scale - final_noise_scale) * (1 - progress) + final_noise_scale
            
            # Reset ç’°å¢ƒ
            reset_seed = seeder.get_reset_seed()
            observation, info = env.reset(seed=reset_seed)
            
            done = False
            truncated = False
            total_reward = 0
            step = 0
            
            while not done and not truncated:
                # é¸æ“‡å‹•ä½œ
                action = algo.select_action(observation, noise_scale=noise_scale)
                
                # åŸ·è¡Œå‹•ä½œ
                next_observation, reward, done, truncated, info = env.step(action)
                
                # å­˜å„²ç¶“é©—
                if hasattr(algo, 'store_experience'):
                    algo.store_experience(observation, action, reward, next_observation, done)
                
                # å®šæœŸæ›´æ–°æ¨¡åž‹
                if step % update_frequency == 0:
                    algo.update_model()
                
                observation = next_observation
                total_reward += reward
                step += 1
            
            episode_rewards.append(total_reward)
            
            # å›žå ±é€²åº¦
            if progress_queue is not None:
                progress_queue.put({
                    'agent_id': agent_id,
                    'agent_name': agent_name,
                    'episode': episode + 1,
                    'reward': total_reward,
                    'avg_reward': np.mean(episode_rewards[-10:])
                })
        
        # å„²å­˜æ¨¡åž‹
        model_save_path = agent_config['model_save_path']
        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
        algo.save_model(model_save_path)
        
        # å›žå‚³çµæžœ
        result_queue.put({
            'agent_id': agent_id,
            'agent_name': agent_name,
            'status': 'success',
            'model_path': model_save_path,
            'episode_rewards': episode_rewards,
            'final_avg_reward': np.mean(episode_rewards[-10:])
        })
        
    except Exception as e:
        import traceback
        result_queue.put({
            'agent_id': agent_config.get('agent_id', -1),
            'agent_name': agent_config.get('name', 'Unknown'),
            'status': 'error',
            'error': str(e),
            'traceback': traceback.format_exc()
        })


class ParallelSubAgentTrainer:
    """
    å¹³è¡ŒåŒ– Sub-Agent è¨“ç·´å™¨
    
    ä½¿ç”¨ multiprocessing åŒæ™‚è¨“ç·´å¤šå€‹ Sub-Agent
    """
    
    def __init__(self, config: Dict, seed: int = 42):
        """
        åˆå§‹åŒ–å¹³è¡Œè¨“ç·´å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
            seed: åŸºç¤Žéš¨æ©Ÿç¨®å­
        """
        self.config = config
        self.seed = seed
        self.sub_agents_config = config['agent_mode'].get('sub_agents', [])
        self.trained_models: Dict[str, str] = {}  # agent_name -> model_path
        
    def train_sub_agents_parallel(self, num_workers: int = None) -> Dict[str, Dict]:
        """
        å¹³è¡Œè¨“ç·´æ‰€æœ‰ Sub-Agents
        
        Args:
            num_workers: æœ€å¤§ä¸¦è¡Œæ•¸ï¼ˆé è¨­ç‚º Sub-Agent æ•¸é‡ï¼‰
        
        Returns:
            è¨“ç·´çµæžœå­—å…¸
        """
        # ä½¿ç”¨ spawn context ä»¥æ”¯æ´ CUDA
        ctx = mp.get_context('spawn')
        
        if num_workers is None:
            num_workers = len(self.sub_agents_config)
        
        # é™åˆ¶æœ€å¤§ worker æ•¸
        num_workers = min(num_workers, mp.cpu_count(), len(self.sub_agents_config))
        
        print(f"\n{'='*70}")
        print(f"[ParallelTrainer] ðŸš€ é–‹å§‹å¹³è¡Œè¨“ç·´ {len(self.sub_agents_config)} å€‹ Sub-Agents")
        print(f"  - ä¸¦è¡Œæ•¸: {num_workers}")
        print(f"  - åŸºç¤Žç¨®å­: {self.seed}")
        print(f"  - CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  - GPU æ•¸é‡: {torch.cuda.device_count()}")
        print(f"{'='*70}\n")
        
        # æº–å‚™æ¯å€‹ Sub-Agent çš„é…ç½®
        agent_configs = self._prepare_agent_configs()
        
        # ä½¿ç”¨ spawn context å‰µå»º Queue
        result_queue = ctx.Queue()
        progress_queue = ctx.Queue()
        
        # å‰µå»ºä¸¦å•Ÿå‹•é€²ç¨‹
        processes = []
        for config in agent_configs:
            p = ctx.Process(
                target=train_single_sub_agent,
                args=(config, result_queue, progress_queue)
            )
            processes.append(p)
            p.start()
        
        # ç›£æŽ§é€²åº¦
        self._monitor_progress(
            processes, 
            progress_queue, 
            len(agent_configs),
            agent_configs[0]['max_episodes']
        )
        
        # ç­‰å¾…æ‰€æœ‰é€²ç¨‹å®Œæˆ
        for p in processes:
            p.join()
        
        # æ”¶é›†çµæžœ
        results = {}
        while not result_queue.empty():
            result = result_queue.get()
            agent_name = result['agent_name']
            results[agent_name] = result
            
            if result['status'] == 'success':
                self.trained_models[agent_name] = result['model_path']
                print(f"  âœ“ {agent_name}: è¨“ç·´å®Œæˆ (avg reward: {result['final_avg_reward']:.2f})")
            else:
                print(f"  âœ— {agent_name}: è¨“ç·´å¤±æ•— - {result.get('error', 'Unknown error')}")
                if 'traceback' in result:
                    print(f"    Traceback: {result['traceback'][:500]}")
        
        print(f"\n{'='*70}")
        print(f"[ParallelTrainer] âœ… æ‰€æœ‰ Sub-Agents è¨“ç·´å®Œæˆï¼")
        print(f"{'='*70}\n")
        
        return results
    
    def _prepare_agent_configs(self) -> List[Dict]:
        """æº–å‚™æ¯å€‹ Sub-Agent çš„é…ç½®"""
        data_cfg = self.config['data']
        env_cfg = self.config['env']
        train_cfg = self.config['training']
        hyper_cfg = self.config['hyperparameters']
        
        stock_symbols = data_cfg['ticker_list']
        
        # æ±ºå®šæ¯å€‹ Agent ä½¿ç”¨çš„è¨­å‚™
        num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        configs = []
        for i, sub_agent in enumerate(self.sub_agents_config):
            agent_seed = self.seed + i
            
            # åˆ†é… GPUï¼ˆå¦‚æžœæœ‰å¤šå€‹ GPUï¼Œè¼ªæµåˆ†é…ï¼‰
            if num_gpus > 0:
                device = f'cuda:{i % num_gpus}'
            else:
                device = 'cpu'
            
            config = {
                'agent_id': i,
                'name': sub_agent.get('name', f'Sub-Agent-{i}'),
                'algorithm': sub_agent.get('algorithm', 'a2c'),
                'model_type': sub_agent.get('model_type', 'mlp'),
                'agent_type': sub_agent.get('agent_type', 'direction'),
                'seed': agent_seed,
                'device': device,
                'max_episodes': train_cfg['max_episodes'],
                'update_frequency': train_cfg['update_frequency'],
                'initial_noise_scale': 0.3,
                'final_noise_scale': 0.01,
                'model_save_path': f"./models/{sub_agent.get('name', f'Sub-Agent-{i}')}_agent.pth",
                'env_config': {
                    'num_stocks': len(stock_symbols),
                    'stock_symbols': stock_symbols,
                    'initial_balance': env_cfg['initial_balance'],
                    'max_steps': env_cfg['max_steps'],
                    'start_date': data_cfg['date_start'],
                    'end_date': data_cfg['date_end'],
                    'transaction_cost': env_cfg['transaction_cost'],
                    'seed': agent_seed,
                    'agent_type': sub_agent.get('agent_type', 'direction'),
                },
                'algo_kwargs': {
                    'actor_lr': float(hyper_cfg['actor_lr']),
                    'critic_lr': float(hyper_cfg['critic_lr']),
                    'gamma': float(hyper_cfg['gamma']),
                    'hidden_dim': int(hyper_cfg['hidden_dim']),
                    'batch_size': int(hyper_cfg['batch_size']),
                    'device': device,
                }
            }
            configs.append(config)
        
        return configs
    
    def _monitor_progress(self, processes: List[Process], progress_queue: Queue,
                         num_agents: int, max_episodes: int):
        """ç›£æŽ§è¨“ç·´é€²åº¦"""
        from collections import defaultdict
        
        progress = defaultdict(lambda: {'episode': 0, 'reward': 0, 'avg_reward': 0})
        
        # å‰µå»ºé€²åº¦æ¢
        pbar = tqdm(
            total=num_agents * max_episodes,
            desc="Training Sub-Agents",
            unit="ep",
            colour='cyan'
        )
        
        completed_episodes = 0
        
        while any(p.is_alive() for p in processes):
            try:
                # éžé˜»å¡žè®€å–
                while not progress_queue.empty():
                    update = progress_queue.get_nowait()
                    agent_name = update['agent_name']
                    progress[agent_name] = update
                    
                    pbar.update(1)
                    completed_episodes += 1
                    
                    # æ›´æ–°é€²åº¦æ¢æè¿°
                    status_str = " | ".join([
                        f"{name[:10]}: E{p['episode']}" 
                        for name, p in progress.items()
                    ])
                    pbar.set_postfix_str(status_str[:60])
                    
            except Exception:
                pass
            
            import time
            time.sleep(0.1)
        
        # æ¸…ç©ºå‰©é¤˜çš„é€²åº¦æ›´æ–°
        while not progress_queue.empty():
            try:
                progress_queue.get_nowait()
                pbar.update(1)
            except Exception:
                break
        
        pbar.close()


class SubAgentEnsemble:
    """
    Sub-Agent é›†æˆå™¨
    
    è¼‰å…¥è¨“ç·´å¥½çš„ Sub-Agent æ¨¡åž‹ï¼Œä¸¦ç”¢ç”Ÿ Q-values ä½œç‚º Final Agent çš„è¼¸å…¥
    
    åŠŸèƒ½ï¼š
    - è¼‰å…¥å¤šå€‹ Sub-Agent æ¨¡åž‹
    - åŸ·è¡Œç‹€æ…‹é©é…ï¼ˆå¾žå®Œæ•´ç‰¹å¾µç¸®å°åˆ°å„ agent éœ€è¦çš„ç‰¹å¾µï¼‰
    - ç”Ÿæˆ Q-values ä¾› Final Agent ä½¿ç”¨
    """
    
    def __init__(self, model_paths: Dict[str, Dict], base_env: TradingEnv = None, device: str = None):
        """
        åˆå§‹åŒ–é›†æˆå™¨
        
        Args:
            model_paths: {agent_name: {
                'path': str,                # æ¨¡åž‹æª”æ¡ˆè·¯å¾‘
                'algorithm': str,           # æ¼”ç®—æ³•åç¨±
                'state_dim': int,           # Sub-Agent æœŸæœ›çš„ç‹€æ…‹ç¶­åº¦
                'action_dim': int,          # å‹•ä½œç¶­åº¦
                'agent_type': str,          # Agent é¡žåž‹ï¼ˆdirection/fundamental/risk_regimeï¼‰
                'model_type': str,          # æ¨¡åž‹é¡žåž‹
                'hidden_dim': int           # éš±è—å±¤ç¶­åº¦
            }}
            base_env: åŸºç¤Ž TradingEnvï¼ˆç”¨æ–¼ç‹€æ…‹é©é…å’Œç‰¹å¾µé…ç½®ï¼‰
            device: è¨ˆç®—è¨­å‚™ï¼ˆNone è¡¨ç¤ºè‡ªå‹•é¸æ“‡ï¼‰
        """
        # è‡ªå‹•é¸æ“‡è¨­å‚™
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        self.sub_agents: Dict[str, AlgorithmStrategy] = {}
        self.agent_info = model_paths
        self.base_env = base_env
        
        # å­˜å„²æ¯å€‹ agent çš„ feature_config
        self.agent_feature_configs: Dict[str, dict] = {}
        
        # å­˜å„²æ¯å€‹ agent çš„ç‹€æ…‹ç¶­åº¦ï¼ˆç”¨æ–¼é©—è­‰ï¼‰
        self.agent_state_dims: Dict[str, int] = {}
        
        # è¨ˆç®—åŸºç¤Žç‰¹å¾µç¶­åº¦ï¼ˆç”¨æ–¼ç‹€æ…‹é©é…ï¼‰
        if base_env is not None:
            self.base_state_features = self._calculate_base_features_dim()
        else:
            self.base_state_features = None
        
        print(f"[SubAgentEnsemble] ä½¿ç”¨è¨­å‚™: {self.device}")
        self._load_models(model_paths)
    
    def _calculate_base_features_dim(self) -> Dict[str, int]:
        """
        è¨ˆç®—åŸºç¤Žç‹€æ…‹ä¸­å„é¡žç‰¹å¾µçš„ç¶­åº¦
        
        Returns:
            {
                'stock': int,       # OHLCV ç‰¹å¾µæ•¸
                'technical': int,   # æŠ€è¡“æŒ‡æ¨™æ•¸
                'fundamental': int, # åŸºæœ¬é¢æ•¸
                'portfolio': int    # æŠ•è³‡çµ„åˆç‹€æ…‹æ•¸ï¼ˆbalance + holdings + portfolio_valueï¼‰
            }
        """
        features_dim = {}
        
        # Stock features (OHLCV)
        if len(self.base_env.stock_data.shape) == 3:
            # (timesteps, num_stocks, features)
            features_dim['stock'] = self.base_env.stock_data.shape[1] * self.base_env.stock_data.shape[2]
        else:
            features_dim['stock'] = self.base_env.stock_data.shape[1]
        
        # Technical indicators
        if len(self.base_env.technical_indicators.shape) == 3:
            features_dim['technical'] = (self.base_env.technical_indicators.shape[1] * 
                                        self.base_env.technical_indicators.shape[2])
        else:
            features_dim['technical'] = self.base_env.technical_indicators.shape[1]
        
        # Fundamental data
        if len(self.base_env.fundamental_data.shape) == 3:
            features_dim['fundamental'] = (self.base_env.fundamental_data.shape[1] * 
                                          self.base_env.fundamental_data.shape[2])
        else:
            features_dim['fundamental'] = self.base_env.fundamental_data.shape[1]
        
        # Portfolio state (balance + holdings + portfolio_value)
        features_dim['portfolio'] = 1 + self.base_env.num_stocks + 1
        
        return features_dim
    
    def _get_agent_feature_config(self, agent_type: str) -> dict:
        """
        æ ¹æ“š agent_type å–å¾—è©² agent æ‡‰è©²ä½¿ç”¨çš„ç‰¹å¾µé…ç½®
        
        Args:
            agent_type: 'direction', 'fundamental', 'risk_regime', 'final'
        
        Returns:
            feature_config: {
                'use_stock': bool,
                'use_technical': bool,
                'use_fundamental': bool,
                'use_portfolio': bool
            }
        """
        if agent_type == 'direction':
            return {
                'use_stock': True,
                'use_technical': True,
                'use_fundamental': False,
                'use_portfolio': True
            }
        elif agent_type == 'fundamental':
            return {
                'use_stock': True,
                'use_technical': False,
                'use_fundamental': True,
                'use_portfolio': True
            }
        elif agent_type == 'risk_regime':
            return {
                'use_stock': True,
                'use_technical': True,
                'use_fundamental': False,
                'use_portfolio': True
            }
        else:  # 'final' or others
            return {
                'use_stock': True,
                'use_technical': True,
                'use_fundamental': True,
                'use_portfolio': True
            }
    
    def _get_agent_state(self, full_state: np.ndarray, agent_name: str) -> np.ndarray:
        """
        å¾žå®Œæ•´ç‹€æ…‹é©é…åˆ°ç‰¹å®š Sub-Agent éœ€è¦çš„ç‹€æ…‹
        
        Args:
            full_state: å®Œæ•´ç‹€æ…‹å‘é‡ï¼ˆåŒ…å«æ‰€æœ‰ç‰¹å¾µï¼‰
            agent_name: Agent åç¨±
        
        Returns:
            é©é…å¾Œçš„ç‹€æ…‹å‘é‡
        """
        if self.base_state_features is None or agent_name not in self.agent_feature_configs:
            # ç„¡æ³•é©é…ï¼Œç›´æŽ¥è¿”å›ž
            return full_state
        
        feature_config = self.agent_feature_configs[agent_name]
        features_dim = self.base_state_features
        
        # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„ä½ç½®
        features_list = []
        offset = 0
        
        # Stock features
        if feature_config['use_stock']:
            stock_dim = features_dim['stock']
            features_list.append(full_state[offset:offset + stock_dim])
            offset += stock_dim
        else:
            offset += features_dim['stock']
        
        # Technical indicators
        if feature_config['use_technical']:
            tech_dim = features_dim['technical']
            features_list.append(full_state[offset:offset + tech_dim])
            offset += tech_dim
        else:
            offset += features_dim['technical']
        
        # Fundamental data
        if feature_config['use_fundamental']:
            fund_dim = features_dim['fundamental']
            features_list.append(full_state[offset:offset + fund_dim])
            offset += fund_dim
        else:
            offset += features_dim['fundamental']
        
        # Portfolio state
        if feature_config['use_portfolio']:
            port_dim = features_dim['portfolio']
            features_list.append(full_state[offset:offset + port_dim])
            # offset += port_dim (ä¸éœ€è¦å†ç”¨)
        
        # æ‹¼æŽ¥é©é…å¾Œçš„ç‹€æ…‹
        if features_list:
            adapted_state = np.concatenate(features_list)
        else:
            # å¦‚æžœæ²’æœ‰ä»»ä½•ç‰¹å¾µè¢«é¸ä¸­ï¼Œè¿”å›žç©ºç‹€æ…‹
            adapted_state = np.array([], dtype=np.float32)
        
        return adapted_state.astype(np.float32)
    
    def _load_models(self, model_paths: Dict[str, Dict]):
        """è¼‰å…¥æ‰€æœ‰ Sub-Agent æ¨¡åž‹"""
        print(f"\n[SubAgentEnsemble] è¼‰å…¥ {len(model_paths)} å€‹ Sub-Agent æ¨¡åž‹...")
        
        for agent_name, info in model_paths.items():
            try:
                # ç²å– agent_type ä¸¦å­˜å„² feature_config
                agent_type = info.get('agent_type', 'direction')
                self.agent_feature_configs[agent_name] = self._get_agent_feature_config(agent_type)
                self.agent_state_dims[agent_name] = info['state_dim']
                
                # å‰µå»ºæ¼”ç®—æ³•æ™‚æŒ‡å®šè¨­å‚™
                algo = AlgorithmFactory.create(
                    info['algorithm'],
                    state_dim=info['state_dim'],
                    action_dim=info['action_dim'],
                    model_type=info.get('model_type', 'mlp'),
                    hidden_dim=info.get('hidden_dim', 256),
                    device=str(self.device)  # å‚³éžè¨­å‚™å­—ä¸²
                )
                
                # è¼‰å…¥æ¨¡åž‹æ¬Šé‡
                algo.load_model(info['path'])
                
                # ç¢ºä¿æ¨¡åž‹åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Šä¸¦è¨­ç‚ºè©•ä¼°æ¨¡å¼
                self._move_algo_to_device(algo)
                
                self.sub_agents[agent_name] = algo
                print(f"  âœ“ {agent_name} ({agent_type}): è¼‰å…¥æˆåŠŸ (è¨­å‚™: {self.device}, state_dim: {info['state_dim']})")
            except Exception as e:
                import traceback
                print(f"  âœ— {agent_name}: è¼‰å…¥å¤±æ•— - {e}")
                traceback.print_exc()
    
    def _move_algo_to_device(self, algo: AlgorithmStrategy):
        """å°‡æ¼”ç®—æ³•çš„æ‰€æœ‰æ¨¡åž‹ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™ä¸¦è¨­ç‚ºè©•ä¼°æ¨¡å¼"""
        # ç§»å‹• Actor
        if hasattr(algo, 'actor') and algo.actor is not None:
            algo.actor = algo.actor.to(self.device)
            algo.actor.eval()
        
        # ç§»å‹• Critic
        if hasattr(algo, 'critic') and algo.critic is not None:
            algo.critic = algo.critic.to(self.device)
            algo.critic.eval()
        
        # ç§»å‹• Target Actor
        if hasattr(algo, 'target_actor') and algo.target_actor is not None:
            algo.target_actor = algo.target_actor.to(self.device)
            algo.target_actor.eval()
        
        # ç§»å‹• Target Critic
        if hasattr(algo, 'target_critic') and algo.target_critic is not None:
            algo.target_critic = algo.target_critic.to(self.device)
            algo.target_critic.eval()
        
        # ç§»å‹• Q Network (for DQN/DDQN)
        if hasattr(algo, 'q_network') and algo.q_network is not None:
            algo.q_network = algo.q_network.to(self.device)
            algo.q_network.eval()
        
        if hasattr(algo, 'target_q_network') and algo.target_q_network is not None:
            algo.target_q_network = algo.target_q_network.to(self.device)
            algo.target_q_network.eval()
    
    def get_q_values(self, state: np.ndarray) -> np.ndarray:
        """
        ç²å–æ‰€æœ‰ Sub-Agent çš„ Q-valuesï¼ˆåŸ·è¡Œç‹€æ…‹é©é…ï¼‰
        
        Args:
            state: å®Œæ•´ç‹€æ…‹å‘é‡ï¼ˆåŒ…å«æ‰€æœ‰ç‰¹å¾µï¼‰
        
        Returns:
            q_values: (num_sub_agents * output_dim,) å±•å¹³çš„ Q-values
        """
        all_q_values = []
        
        for agent_name, algo in self.sub_agents.items():
            # â˜… é—œéµï¼šåŸ·è¡Œç‹€æ…‹é©é…
            adapted_state = self._get_agent_state(state, agent_name)
            
            # é©—è­‰é©é…å¾Œçš„ç‹€æ…‹ç¶­åº¦
            expected_dim = self.agent_state_dims.get(agent_name, -1)
            if len(adapted_state) != expected_dim and expected_dim > 0:
                print(f"  âš  {agent_name}: ç‹€æ…‹ç¶­åº¦ä¸åŒ¹é… (æœŸæœ›: {expected_dim}, å¯¦éš›: {len(adapted_state)})")
            
            # ç²å– Q-values
            q_values = self._get_agent_q_values(algo, adapted_state)
            all_q_values.append(q_values)
        
        # å±•å¹³æ‰€æœ‰ Q-values
        if all_q_values:
            return np.concatenate(all_q_values)
        else:
            return np.array([])
    
    def _get_agent_q_values(self, algo: AlgorithmStrategy, state: np.ndarray) -> np.ndarray:
        """
        ç²å–å–®å€‹ Agent çš„ Q-values
        
        ä¸åŒæ¼”ç®—æ³•æœ‰ä¸åŒçš„ç²å–æ–¹å¼
        """
        # ç¢ºä¿ state æ˜¯æ­£ç¢ºçš„æ ¼å¼
        if isinstance(state, np.ndarray):
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        else:
            state_tensor = state.unsqueeze(0).to(self.device) if state.dim() == 1 else state.to(self.device)
        
        with torch.no_grad():
            if hasattr(algo, 'q_network') and algo.q_network is not None:
                # DDQN: ç›´æŽ¥å¾ž Q-network ç²å–
                q_values = algo.q_network(state_tensor).cpu().numpy().flatten()
            elif hasattr(algo, 'actor') and algo.actor is not None:
                # A2C/DDPG: å¾ž actor ç²å– logits/actions
                logits = algo.actor(state_tensor).cpu().numpy().flatten()
                
                if hasattr(algo, 'critic') and algo.critic is not None:
                    # å˜—è©¦ç²å– critic value
                    try:
                        # å°æ–¼ A2Cï¼Œcritic åªæŽ¥å— state
                        value = algo.critic(state_tensor).cpu().numpy().flatten()
                        q_values = np.concatenate([logits, value])
                    except Exception:
                        # å°æ–¼ DDPGï¼Œcritic éœ€è¦ state å’Œ action
                        q_values = logits
                else:
                    q_values = logits
            elif hasattr(algo, 'critic') and algo.critic is not None:
                q_values = algo.critic(state_tensor).cpu().numpy().flatten()
            else:
                # é è¨­ï¼šå›žå‚³é›¶å‘é‡
                action_dim = getattr(algo, 'action_dim', 10)
                q_values = np.zeros(action_dim)
        
        return q_values
    
    def get_ensemble_actions(self, state: np.ndarray) -> Dict[str, np.ndarray]:
        """
        ç²å–æ‰€æœ‰ Sub-Agent çš„å‹•ä½œå»ºè­°ï¼ˆåŸ·è¡Œç‹€æ…‹é©é…ï¼‰
        
        Args:
            state: å®Œæ•´ç‹€æ…‹å‘é‡
        
        Returns:
            actions: {agent_name: action_array}
        """
        actions = {}
        for agent_name, algo in self.sub_agents.items():
            try:
                # â˜… åŸ·è¡Œç‹€æ…‹é©é…
                adapted_state = self._get_agent_state(state, agent_name)
                
                # ä½¿ç”¨ç¢ºå®šæ€§å‹•ä½œé¸æ“‡
                if hasattr(algo, 'select_action_deterministic'):
                    action = algo.select_action_deterministic(adapted_state)
                else:
                    action = algo.select_action(adapted_state, noise_scale=0.0)
                actions[agent_name] = action
            except Exception as e:
                print(f"  âš  {agent_name} é¸æ“‡å‹•ä½œå¤±æ•—: {e}")
                actions[agent_name] = np.zeros(algo.action_dim)
        return actions
    
    def get_q_values_dim(self) -> int:
        """
        ç²å– Q-values çš„ç¸½ç¶­åº¦
        
        é€éŽå¯¦éš›è¨ˆç®—ä¾†ç²å–æº–ç¢ºçš„ç¶­åº¦
        """
        # å˜—è©¦ç”¨ä¸€å€‹å‡çš„ state ä¾†è¨ˆç®—å¯¦éš›ç¶­åº¦
        if not self.sub_agents or self.base_env is None:
            return self._estimate_q_values_dim()
        
        # ç²å–åŸºç¤Žç’°å¢ƒçš„å®Œæ•´ç‹€æ…‹ç¶­åº¦
        full_state_dim = self.base_env.state_dim
        
        # å‰µå»ºä¸€å€‹å‡çš„å®Œæ•´ç‹€æ…‹
        dummy_state = np.zeros(full_state_dim)
        
        try:
            # å¯¦éš›è¨ˆç®— Q-values ç¶­åº¦
            q_values = self.get_q_values(dummy_state)
            actual_dim = len(q_values)
            print(f"[SubAgentEnsemble] å¯¦éš› Q-values ç¶­åº¦: {actual_dim}")
            return actual_dim
        except Exception as e:
            print(f"[SubAgentEnsemble] ç„¡æ³•è¨ˆç®—å¯¦éš›ç¶­åº¦ï¼Œä½¿ç”¨ä¼°ç®—: {e}")
            # å›žé€€åˆ°ä¼°ç®—æ–¹æ³•
            return self._estimate_q_values_dim()
    
    def _estimate_q_values_dim(self) -> int:
        """ä¼°ç®— Q-values ç¶­åº¦ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
        total_dim = 0
        for agent_name, algo in self.sub_agents.items():
            action_dim = getattr(algo, 'action_dim', 10)
            
            if hasattr(algo, 'q_network') and algo.q_network is not None:
                # DDQN: Q-network è¼¸å‡ºç¶­åº¦
                total_dim += action_dim * 3  # 3 = buy, hold, sell
            elif hasattr(algo, 'actor') and algo.actor is not None:
                # Actor-Critic æ–¹æ³•
                actor_output_dim = action_dim * 3  # A2C è¼¸å‡ºæ˜¯ action_dim * 3
                
                if hasattr(algo, 'critic') and algo.critic is not None:
                    critic_output_dim = 1
                    total_dim += actor_output_dim + critic_output_dim
                else:
                    total_dim += actor_output_dim
            else:
                total_dim += action_dim
        
        return total_dim