import os
import json
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

class TrainingLogger:
    """
    è¨“ç·´æ—¥èªŒè¨˜éŒ„å™¨
    
    è¨˜éŒ„è¨“ç·´éç¨‹ä¸­çš„å„ç¨®æŒ‡æ¨™ï¼Œä¸¦ç¹ªè£½åœ–è¡¨
    """
    
    def __init__(self, 
                 agent_name: str,
                 algorithm: str,
                 agent_mode: str = 'single-agent',
                 log_dir: str = './logs',
                 save_frequency: int = 10):
        """
        åˆå§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨
        
        :param agent_name: Agent åç¨±
        :param algorithm: æ¼”ç®—æ³•åç¨±
        :param agent_mode: Agent æ¨¡å¼ (single-agent / multi-agent)
        :param log_dir: æ—¥èªŒç›®éŒ„
        :param save_frequency: å„²å­˜é »ç‡ï¼ˆæ¯ N å€‹ episodeï¼‰
        """
        self.agent_name = agent_name
        self.algorithm = algorithm.upper()
        self.agent_mode = agent_mode
        self.save_frequency = save_frequency
        
        # å‰µå»ºå¸¶æ™‚é–“æˆ³çš„æ—¥èªŒç›®éŒ„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{algorithm}_{agent_mode}_{timestamp}"
        self.log_dir = os.path.join(log_dir, self.run_name)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(os.path.join(self.log_dir, 'plots'), exist_ok=True)
        
        # ==================== è¨“ç·´æŒ‡æ¨™ ====================
        self.training_metrics = {
            # æ¯ Episode æŒ‡æ¨™
            'episode': [],
            'episode_reward': [],
            'episode_length': [],
            'cumulative_reward': [],
            
            # æå¤±å‡½æ•¸
            'actor_loss': [],
            'critic_loss': [],
            
            # æ¢ç´¢ç‡
            'noise_scale': [],
            
            # æ™‚é–“æˆ³
            'timesteps': [],
            'wall_time': [],
        }
        
        # ==================== äº¤æ˜“ç¸¾æ•ˆæŒ‡æ¨™ ====================
        self.trading_metrics = {
            'episode': [],
            'initial_portfolio': [],
            'final_portfolio': [],
            'total_return': [],
            'sharpe_ratio': [],
            'max_drawdown': [],
            'volatility': [],
            'win_rate': [],
            'num_trades': [],
        }
        
        # ==================== å‹•ä½œåˆ†ä½ˆçµ±è¨ˆ ====================
        self.action_metrics = {
            'episode': [],
            'buy_count': [],
            'hold_count': [],
            'sell_count': [],
            'buy_ratio': [],
            'hold_ratio': [],
            'sell_ratio': [],
        }
        
        # ==================== æ¯æ­¥è©³ç´°è¨˜éŒ„ï¼ˆå¯é¸ï¼‰ ====================
        self.step_metrics = {
            'episode': [],
            'step': [],
            'reward': [],
            'portfolio_value': [],
            'balance': [],
            'action': [],  # å­˜å„²å‹•ä½œé™£åˆ—
        }
        
        # å…§éƒ¨è¿½è¹¤è®Šæ•¸
        self._episode_start_time = None
        self._current_episode = 0
        self._total_timesteps = 0
        self._cumulative_reward = 0.0
        self._episode_actions = []
        self._episode_rewards = []
        self._episode_portfolio_values = []
        
        print(f"[TrainingLogger] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ—¥èªŒç›®éŒ„: {self.log_dir}")
        print(f"  - Agent: {agent_name}")
        print(f"  - æ¼”ç®—æ³•: {self.algorithm}\n")
    
    # ==================== Episode ç´šåˆ¥è¨˜éŒ„ ====================
    
    def start_episode(self, episode: int):
        """é–‹å§‹æ–°çš„ Episode"""
        self._episode_start_time = datetime.now()
        self._current_episode = episode
        self._episode_actions = []
        self._episode_rewards = []
        self._episode_portfolio_values = []
    
    def log_step(self, 
                 step: int,
                 action: np.ndarray,
                 reward: float,
                 portfolio_value: float,
                 balance: float,
                 info: Dict = None):
        """
        è¨˜éŒ„å–®æ­¥è³‡è¨Š
        
        :param step: ç•¶å‰æ­¥æ•¸
        :param action: å‹•ä½œé™£åˆ— {-1, 0, 1}
        :param reward: çå‹µ
        :param portfolio_value: è³‡ç”¢çµ„åˆåƒ¹å€¼
        :param balance: ç¾é‡‘é¤˜é¡
        :param info: é¡å¤–è³‡è¨Š
        """
        self._episode_actions.append(action.copy())
        self._episode_rewards.append(reward)
        self._episode_portfolio_values.append(portfolio_value)
        self._total_timesteps += 1
    
    def end_episode(self,
                episode_reward: float,
                episode_length: int,
                actor_loss: float,
                critic_loss: float,
                noise_scale: float,
                initial_portfolio: float,
                final_portfolio: float,
                info: Dict = None):
        """
        çµæŸ Episode ä¸¦è¨˜éŒ„å½™ç¸½æŒ‡æ¨™
        """
        self._cumulative_reward += episode_reward
        wall_time = (datetime.now() - self._episode_start_time).total_seconds()
        
        # è¨“ç·´æŒ‡æ¨™
        self.training_metrics['episode'].append(self._current_episode)
        self.training_metrics['episode_reward'].append(episode_reward)
        self.training_metrics['episode_length'].append(episode_length)
        self.training_metrics['cumulative_reward'].append(self._cumulative_reward)
        self.training_metrics['actor_loss'].append(actor_loss)
        self.training_metrics['critic_loss'].append(critic_loss)
        self.training_metrics['noise_scale'].append(noise_scale)
        self.training_metrics['timesteps'].append(self._total_timesteps)
        self.training_metrics['wall_time'].append(wall_time)
        
        # ==================== è¨ˆç®—äº¤æ˜“ç¸¾æ•ˆ ====================
        
        # 1. ç¸½å ±é…¬ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        if initial_portfolio > 0:
            total_return = (final_portfolio - initial_portfolio) / initial_portfolio * 100
        else:
            total_return = 0.0
        
        # 2. Sharpe Ratioï¼ˆå¹´åŒ–ï¼‰
        sharpe_ratio = 0.0
        volatility = 0.0
        
        if len(self._episode_portfolio_values) > 1:
            # è¨ˆç®—æ—¥æ”¶ç›Šç‡
            portfolio_values = np.array(self._episode_portfolio_values, dtype=np.float64)
            daily_returns = np.diff(portfolio_values) / portfolio_values[:-1]  # ä¸åš * 100
            
            daily_mean = np.mean(daily_returns)
            daily_std = np.std(daily_returns)
            
            # â˜… ä¿®å¾©ï¼šSharpe Ratio æ­£ç¢ºè¨ˆç®—
            # Sharpe = (avg_return - risk_free_rate) / std_return * sqrt(252)
            risk_free_daily = 0.02 / 252  # å¹´åŒ– 2% è½‰æ›ç‚ºæ—¥ç‡
            
            if daily_std > 1e-8:  # é¿å…é™¤ä»¥ 0
                sharpe_ratio = (daily_mean - risk_free_daily) / daily_std * np.sqrt(252)
            else:
                sharpe_ratio = 0.0
            
            # â˜… ä¿®å¾©ï¼šå¹´åŒ–æ³¢å‹•ç‡ï¼ˆæ­£ç¢ºè½‰æ›ï¼‰
            volatility = daily_std * np.sqrt(252) * 100  # æœ€å¾Œæ‰ * 100
        
        # 3. æœ€å¤§å›æ’¤ï¼ˆç›¸å°æœ€é«˜é»çš„ä¸‹è·Œï¼‰
        max_drawdown = 0.0
        if len(self._episode_portfolio_values) > 0:
            portfolio_values = np.array(self._episode_portfolio_values, dtype=np.float64)
            peak = np.maximum.accumulate(portfolio_values)
            
            # â˜… ä¿®å¾©ï¼šé¿å…é™¤ä»¥ 0ï¼ˆpeak å¯èƒ½ç‚º 0ï¼‰
            with np.errstate(divide='ignore', invalid='ignore'):
                drawdown = (peak - portfolio_values) / peak
                drawdown = np.where(np.isfinite(drawdown), drawdown, 0)  # NaN è½‰ç‚º 0
            
            max_drawdown = np.max(drawdown) * 100
        
        # 4. å‹ç‡ï¼ˆçå‹µ > 0 çš„æ­¥æ•¸æ¯”ä¾‹ï¼‰
        win_rate = 0.0
        if len(self._episode_rewards) > 0:
            positive_rewards = np.sum(np.array(self._episode_rewards) > 0)
            win_rate = positive_rewards / len(self._episode_rewards) * 100
        
        # 5. äº¤æ˜“æ¬¡æ•¸ï¼ˆå¤šè‚¡ç¥¨æ™‚è¦è€ƒæ…®ç¶­åº¦ï¼‰
        num_trades = 0
        if len(self._episode_actions) > 0:
            all_actions = np.array(self._episode_actions)
            # è¨ˆç®—æ‰€æœ‰éæŒæœ‰å‹•ä½œçš„ç¸½æ•¸
            num_trades = np.sum(all_actions != 0)
        
        # ==================== è¨˜éŒ„äº¤æ˜“ç¸¾æ•ˆ ====================
        
        self.trading_metrics['episode'].append(self._current_episode)
        self.trading_metrics['initial_portfolio'].append(initial_portfolio)
        self.trading_metrics['final_portfolio'].append(final_portfolio)
        self.trading_metrics['total_return'].append(total_return)
        self.trading_metrics['sharpe_ratio'].append(sharpe_ratio)
        self.trading_metrics['max_drawdown'].append(max_drawdown)
        self.trading_metrics['volatility'].append(volatility)
        self.trading_metrics['win_rate'].append(win_rate)
        self.trading_metrics['num_trades'].append(num_trades)
        
        # ==================== å‹•ä½œåˆ†ä½ˆçµ±è¨ˆ ====================
        
        if len(self._episode_actions) > 0:
            all_actions = np.array(self._episode_actions).flatten()
            total_actions = len(all_actions)
            
            # â˜… æª¢æ¸¬ä¸¦è½‰æ›å‹•ä½œå€¼
            if np.all((all_actions >= 0) & (all_actions <= 2)):
                all_actions = all_actions - 1  # è½‰æ› [0,1,2] â†’ [-1,0,1]
            
            # è¨ˆç®—å„å‹•ä½œè¨ˆæ•¸
            buy_count = np.sum(all_actions == 1)
            hold_count = np.sum(all_actions == 0)
            sell_count = np.sum(all_actions == -1)
            counted_total = int(buy_count + hold_count + sell_count)
            
            self.action_metrics['episode'].append(self._current_episode)
            self.action_metrics['buy_count'].append(int(buy_count))
            self.action_metrics['hold_count'].append(int(hold_count))
            self.action_metrics['sell_count'].append(int(sell_count))
            
            # è¨ˆç®—æ¯”ä¾‹
            if counted_total > 0:
                buy_ratio = buy_count / counted_total * 100
                hold_ratio = hold_count / counted_total * 100
                sell_ratio = sell_count / counted_total * 100
                
                self.action_metrics['buy_ratio'].append(buy_ratio)
                self.action_metrics['hold_ratio'].append(hold_ratio)
                self.action_metrics['sell_ratio'].append(sell_ratio)
                
                # èª¿è©¦æª¢æŸ¥
                if counted_total != total_actions:
                    print(f"[TrainingLogger] âš ï¸ Episode {self._current_episode}: "
                        f"å‹•ä½œè¨ˆæ•¸ä¸ç¬¦ (è¨ˆæ•¸: {counted_total}, å¯¦éš›: {total_actions})")
        
        # å®šæœŸå„²å­˜
        if self._current_episode % self.save_frequency == 0:
            self.save_metrics()
    
    # ==================== å„²å­˜èˆ‡è¼‰å…¥ ====================
    
    def save_metrics(self):
        """å„²å­˜æ‰€æœ‰æŒ‡æ¨™åˆ° CSV å’Œ JSON"""
        # å„²å­˜è¨“ç·´æŒ‡æ¨™
        training_df = pd.DataFrame(self.training_metrics)
        training_df.to_csv(os.path.join(self.log_dir, 'training_metrics.csv'), index=False)
        
        # å„²å­˜äº¤æ˜“ç¸¾æ•ˆ
        trading_df = pd.DataFrame(self.trading_metrics)
        trading_df.to_csv(os.path.join(self.log_dir, 'trading_metrics.csv'), index=False)
        
        # å„²å­˜å‹•ä½œåˆ†ä½ˆ
        action_df = pd.DataFrame(self.action_metrics)
        action_df.to_csv(os.path.join(self.log_dir, 'action_metrics.csv'), index=False)
        
        # å„²å­˜é…ç½®è³‡è¨Š
        config = {
            'agent_name': self.agent_name,
            'algorithm': self.algorithm,
            'agent_mode': self.agent_mode,
            'total_episodes': self._current_episode,
            'total_timesteps': self._total_timesteps,
            'final_cumulative_reward': self._cumulative_reward,
        }
        with open(os.path.join(self.log_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
    
    # ==================== ç¹ªåœ–åŠŸèƒ½ ====================
    
    def plot_training_curves(self):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        if len(self.training_metrics['episode']) == 0:
            print("[TrainingLogger] æ²’æœ‰è¶³å¤ çš„æ•¸æ“šç¹ªè£½åœ–è¡¨")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle(f'{self.algorithm} Training Curves - {self.agent_name}', fontsize=14)
        
        episodes = self.training_metrics['episode']
        
        # 1. Episode Reward
        ax1 = axes[0, 0]
        ax1.plot(episodes, self.training_metrics['episode_reward'], alpha=0.6, label='Episode Reward')
        # ç§»å‹•å¹³å‡
        if len(episodes) >= 10:
            ma = pd.Series(self.training_metrics['episode_reward']).rolling(10).mean()
            ax1.plot(episodes, ma, linewidth=2, label='MA(10)')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Reward')
        ax1.set_title('Episode Reward')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Cumulative Reward
        ax2 = axes[0, 1]
        ax2.plot(episodes, self.training_metrics['cumulative_reward'], color='green')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Cumulative Reward')
        ax2.set_title('Cumulative Reward')
        ax2.grid(True, alpha=0.3)
        
        # 3. Actor & Critic Loss
        ax3 = axes[0, 2]
        ax3.plot(episodes, self.training_metrics['actor_loss'], label='Actor Loss', alpha=0.7)
        ax3.plot(episodes, self.training_metrics['critic_loss'], label='Critic Loss', alpha=0.7)
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Loss')
        ax3.set_title('Actor & Critic Loss')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Total Return
        ax4 = axes[1, 0]
        returns = self.trading_metrics['total_return']
        colors = ['green' if r >= 0 else 'red' for r in returns]
        ax4.bar(self.trading_metrics['episode'], returns, color=colors, alpha=0.7)
        ax4.axhline(y=0, color='black', linestyle='--', linewidth=0.5)
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Return (%)')
        ax4.set_title('Total Return per Episode')
        ax4.grid(True, alpha=0.3)
        
        # 5. Sharpe Ratio
        ax5 = axes[1, 1]
        ax5.plot(self.trading_metrics['episode'], self.trading_metrics['sharpe_ratio'], color='purple')
        ax5.axhline(y=0, color='black', linestyle='--', linewidth=0.5)
        ax5.set_xlabel('Episode')
        ax5.set_ylabel('Sharpe Ratio')
        ax5.set_title('Sharpe Ratio')
        ax5.grid(True, alpha=0.3)
        
        # 6. Exploration Rate
        ax6 = axes[1, 2]
        ax6.plot(episodes, self.training_metrics['noise_scale'], color='orange')
        ax6.set_xlabel('Episode')
        ax6.set_ylabel('Noise Scale / Epsilon')
        ax6.set_title('Exploration Rate Decay')
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.log_dir, 'plots', 'training_curves.png'), dpi=150)
        plt.close()
        print(f"[TrainingLogger] è¨“ç·´æ›²ç·šå·²å„²å­˜")
    
    def plot_action_distribution(self):
        """ç¹ªè£½å‹•ä½œåˆ†ä½ˆåœ–"""
        if len(self.action_metrics['episode']) == 0:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        fig.suptitle(f'{self.algorithm} Action Distribution - {self.agent_name}', fontsize=14)
        
        episodes = self.action_metrics['episode']
        
        # 1. å †ç–Šé¢ç©åœ–
        ax1 = axes[0]
        ax1.stackplot(episodes,
                      self.action_metrics['sell_ratio'],
                      self.action_metrics['hold_ratio'],
                      self.action_metrics['buy_ratio'],
                      labels=['Sell (-1)', 'Hold (0)', 'Buy (+1)'],
                      colors=['red', 'gray', 'green'],
                      alpha=0.7)
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Percentage (%)')
        ax1.set_title('Action Distribution Over Episodes')
        ax1.legend(loc='upper right')
        ax1.set_ylim(0, 100)
        ax1.grid(True, alpha=0.3)
        
        # 2. æœ€çµ‚åˆ†ä½ˆåœ“é¤…åœ–
        ax2 = axes[1]
        final_buy = np.mean(self.action_metrics['buy_ratio'][-10:]) if len(self.action_metrics['buy_ratio']) >= 10 else self.action_metrics['buy_ratio'][-1]
        final_hold = np.mean(self.action_metrics['hold_ratio'][-10:]) if len(self.action_metrics['hold_ratio']) >= 10 else self.action_metrics['hold_ratio'][-1]
        final_sell = np.mean(self.action_metrics['sell_ratio'][-10:]) if len(self.action_metrics['sell_ratio']) >= 10 else self.action_metrics['sell_ratio'][-1]
        
        sizes = [final_sell, final_hold, final_buy]
        labels = [f'Sell\n{final_sell:.1f}%', f'Hold\n{final_hold:.1f}%', f'Buy\n{final_buy:.1f}%']
        colors = ['#ff6b6b', '#868e96', '#51cf66']
        
        ax2.pie(sizes, labels=labels, colors=colors, autopct='', startangle=90)
        ax2.set_title('Average Action Distribution (Last 10 Episodes)')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.log_dir, 'plots', 'action_distribution.png'), dpi=150)
        plt.close()
        print(f"[TrainingLogger] å‹•ä½œåˆ†ä½ˆåœ–å·²å„²å­˜")
    
    def plot_portfolio_performance(self):
        """ç¹ªè£½æŠ•è³‡çµ„åˆç¸¾æ•ˆåœ–"""
        if len(self.trading_metrics['episode']) == 0:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'{self.algorithm} Portfolio Performance - {self.agent_name}', fontsize=14)
        
        episodes = self.trading_metrics['episode']
        
        # 1. Portfolio Value
        ax1 = axes[0, 0]
        ax1.plot(episodes, self.trading_metrics['final_portfolio'], label='Final Portfolio', color='blue')
        ax1.axhline(y=self.trading_metrics['initial_portfolio'][0], color='red', linestyle='--', label='Initial')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Portfolio Value ($)')
        ax1.set_title('Portfolio Value')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Max Drawdown
        ax2 = axes[0, 1]
        ax2.fill_between(episodes, 0, self.trading_metrics['max_drawdown'], color='red', alpha=0.5)
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Max Drawdown (%)')
        ax2.set_title('Maximum Drawdown')
        ax2.grid(True, alpha=0.3)
        
        # 3. Win Rate
        ax3 = axes[1, 0]
        ax3.plot(episodes, self.trading_metrics['win_rate'], color='green')
        ax3.axhline(y=50, color='black', linestyle='--', linewidth=0.5)
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Win Rate (%)')
        ax3.set_title('Win Rate')
        ax3.set_ylim(0, 100)
        ax3.grid(True, alpha=0.3)
        
        # 4. Number of Trades
        ax4 = axes[1, 1]
        ax4.bar(episodes, self.trading_metrics['num_trades'], color='purple', alpha=0.7)
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Number of Trades')
        ax4.set_title('Trading Activity')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.log_dir, 'plots', 'portfolio_performance.png'), dpi=150)
        plt.close()
        print(f"[TrainingLogger] æŠ•è³‡çµ„åˆç¸¾æ•ˆåœ–å·²å„²å­˜")
    
    def plot_all(self):
        """ç¹ªè£½æ‰€æœ‰åœ–è¡¨"""
        self.plot_training_curves()
        self.plot_action_distribution()
        self.plot_portfolio_performance()
        print(f"[TrainingLogger] æ‰€æœ‰åœ–è¡¨å·²å„²å­˜è‡³ {self.log_dir}/plots/")
    
    # ==================== æ‘˜è¦å ±å‘Š ====================
    
    def print_summary(self):
        """æ‰“å°è¨“ç·´æ‘˜è¦"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š è¨“ç·´æ‘˜è¦ - {self.algorithm} ({self.agent_name})")
        print(f"{'='*70}")
        
        if len(self.training_metrics['episode']) == 0:
            print("  æ²’æœ‰è¨“ç·´æ•¸æ“š")
            return
        
        # è¨“ç·´çµ±è¨ˆ
        print(f"\nğŸ“ˆ è¨“ç·´çµ±è¨ˆ:")
        print(f"  - ç¸½ Episodes: {self._current_episode}")
        print(f"  - ç¸½ Timesteps: {self._total_timesteps}")
        print(f"  - ç´¯ç©çå‹µ: {self._cumulative_reward:.4f}")
        print(f"  - å¹³å‡ Episode çå‹µ: {np.mean(self.training_metrics['episode_reward']):.4f}")
        print(f"  - æœ€é«˜ Episode çå‹µ: {np.max(self.training_metrics['episode_reward']):.4f}")
        print(f"  - æœ€ä½ Episode çå‹µ: {np.min(self.training_metrics['episode_reward']):.4f}")
        
        # äº¤æ˜“ç¸¾æ•ˆ
        print(f"\nğŸ’° äº¤æ˜“ç¸¾æ•ˆ (æœ€å¾Œ 10 Episodes å¹³å‡):")
        last_n = min(10, len(self.trading_metrics['total_return']))
        print(f"  - å¹³å‡å ±é…¬ç‡: {np.mean(self.trading_metrics['total_return'][-last_n:]):.2f}%")
        print(f"  - å¹³å‡ Sharpe Ratio: {np.mean(self.trading_metrics['sharpe_ratio'][-last_n:]):.4f}")
        print(f"  - å¹³å‡æœ€å¤§å›æ’¤: {np.mean(self.trading_metrics['max_drawdown'][-last_n:]):.2f}%")
        print(f"  - å¹³å‡å‹ç‡: {np.mean(self.trading_metrics['win_rate'][-last_n:]):.2f}%")
        
        # å‹•ä½œåˆ†ä½ˆ
        if len(self.action_metrics['buy_ratio']) > 0:
            print(f"\nğŸ¯ å‹•ä½œåˆ†ä½ˆ (æœ€å¾Œ 10 Episodes å¹³å‡):")
            last_n = min(10, len(self.action_metrics['buy_ratio']))
            print(f"  - Buy:  {np.mean(self.action_metrics['buy_ratio'][-last_n:]):.1f}%")
            print(f"  - Hold: {np.mean(self.action_metrics['hold_ratio'][-last_n:]):.1f}%")
            print(f"  - Sell: {np.mean(self.action_metrics['sell_ratio'][-last_n:]):.1f}%")
        
        print(f"\nğŸ“ æ—¥èªŒç›®éŒ„: {self.log_dir}")
        print(f"{'='*70}\n")
    
    def finalize(self):
        """å®Œæˆè¨“ç·´ï¼Œå„²å­˜æ‰€æœ‰æ•¸æ“šå’Œåœ–è¡¨"""
        self.save_metrics()
        self.plot_all()
        self.print_summary()