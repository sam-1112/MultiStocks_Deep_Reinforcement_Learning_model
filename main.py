import yaml
import os
import argparse
from trader.envs.factory import EnvironmentFactory
from trader.trainer import Trainer, HierarchicalTrainer
from trader.utils.seed import SeedManager

def load_config(config_path: str = './configs/defaults.yaml') -> dict:
    """åŠ è¼‰é…ç½®"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Configuration not found: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"[Main] Loaded configuration from {config_path}\n")
    return config

def get_date_ranges(data_cfg: dict) -> tuple:
    """
    å¾é…ç½®ä¸­æå–è¨“ç·´å’Œæ¸¬è©¦çš„æ—¥æœŸç¯„åœ
    
    :param data_cfg: æ•¸æ“šé…ç½®å­—å…¸
    :return: (train_start, train_end, test_start, test_end) å…ƒçµ„
    """
    train_start = data_cfg.get('train_date_start', data_cfg.get('date_start', '2010-01-01'))
    train_end = data_cfg.get('train_date_end', data_cfg.get('date_end', '2021-09-30'))
    test_start = data_cfg.get('test_date_start', data_cfg.get('date_start', '2021-10-01'))
    test_end = data_cfg.get('test_date_end', data_cfg.get('date_end', '2023-03-01'))
    
    return train_start, train_end, test_start, test_end

def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='Multi-Stock Deep Reinforcement Learning Trader')
    
    parser.add_argument('--config', '-c', type=str, default='./configs/defaults.yaml',
                       help='é…ç½®æª”æ¡ˆè·¯å¾‘ (default: ./configs/defaults.yaml)')
    parser.add_argument('--train', action='store_true',
                       help='åŸ·è¡Œè¨“ç·´æ¨¡å¼')
    parser.add_argument('--eval', action='store_true',
                       help='åŸ·è¡Œè©•ä¼°æ¨¡å¼')
    parser.add_argument('--model', '-m', type=str, default=None,
                       help='æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (è©•ä¼°æ™‚ä½¿ç”¨)')
    parser.add_argument('--seed', type=int, default=None,
                       help='éš¨æ©Ÿç¨®å­ (è¦†è“‹é…ç½®æª”æ¡ˆè¨­å®š)')
    parser.add_argument('--num-workers', '-w', type=int, default=None,
                       help='å¹³è¡Œè¨“ç·´çš„ worker æ•¸é‡ (é è¨­: Sub-Agent æ•¸é‡)')
    parser.add_argument('--train-sub-agent', type=int, default=None,
                       help='è¨“ç·´æŒ‡å®šç´¢å¼•çš„ Sub-Agent (0, 1, 2, ...)')
    parser.add_argument('--train-final-only', action='store_true',
                       help='åªè¨“ç·´ Final Agent (ä½¿ç”¨å·²è¨“ç·´çš„ Sub-Agent æ¨¡å‹)')
    
    return parser.parse_args()

def main():
    """ä¸»ç¨‹åº"""
    args = parse_args()
    config = load_config(args.config)
    
    # æ ¹æ“šå‘½ä»¤åˆ—åƒæ•¸æ±ºå®šæ“ä½œæ¨¡å¼
    if args.train:
        config['agent_mode']['operation'] = 'training'
    elif args.eval:
        config['agent_mode']['operation'] = 'evaluation'
    else:
        # é è¨­ç‚ºè¨“ç·´æ¨¡å¼
        config['agent_mode']['operation'] = 'training'
    
    # å¦‚æœå‘½ä»¤åˆ—æŒ‡å®šäº†ç¨®å­ï¼Œè¦†è“‹é…ç½®æª”æ¡ˆ
    if args.seed is not None:
        config['seed'] = args.seed
    
    # â† é©—è­‰å¿…éœ€çš„éµ
    required_keys = {
        'data': ['ticker_list', 'date_start', 'date_end'],
        'env': ['initial_balance', 'max_steps', 'transaction_cost'],
        'training': ['max_episodes', 'update_frequency'],
        'hyperparameters': ['actor_lr', 'critic_lr', 'gamma', 'hidden_dim', 'batch_size'],
        'evaluation': ['num_episodes'], 
        'agent_mode': ['mode']
    }
    
    for section, keys in required_keys.items():
        if section not in config:
            raise KeyError(f"Missing configuration section: {section}")
        for key in keys:
            if key not in config[section]:
                raise KeyError(f"Missing key '{key}' in section '{section}'")
    
    data_cfg = config['data']
    env_cfg = config['env']
    train_cfg = config['training']
    hyper_cfg = config['hyperparameters']
    eval_cfg = config['evaluation']
    agent_mode_cfg = config['agent_mode']
    
    # â˜…â˜…â˜… æ–°å¢ï¼šæå–è¨“ç·´å’Œæ¸¬è©¦çš„æ—¥æœŸç¯„åœ â˜…â˜…â˜…
    train_start, train_end, test_start, test_end = get_date_ranges(data_cfg)
    
    print(f"[Main] ğŸ“… æ—¥æœŸç¯„åœ:")
    print(f"  - è¨“ç·´æœŸé–“: {train_start} è‡³ {train_end}")
    print(f"  - æ¸¬è©¦æœŸé–“: {test_start} è‡³ {test_end}\n")
    
    # â† ç¢ºä¿è¶…åƒæ•¸é¡å‹æ­£ç¢º
    actor_lr = float(hyper_cfg['actor_lr'])
    critic_lr = float(hyper_cfg['critic_lr'])
    gamma = float(hyper_cfg['gamma'])
    hidden_dim = int(hyper_cfg['hidden_dim'])
    batch_size = int(hyper_cfg['batch_size'])
    
    # â† ç²å–ç¨®å­ï¼ˆé»˜èª 42ï¼‰
    seed = config.get('seed', 42)
    
    # â† è¨­ç½®å…¨å±€éš¨æ©Ÿç¨®å­
    SeedManager.set_seed(seed)
    
    stock_symbols = data_cfg['ticker_list']
    
    # ç²å–æ“ä½œæ¨¡å¼
    operation = agent_mode_cfg.get('operation', 'training')

    print(f"\n{'='*70}")
    print(f"[Main] ğŸš€ Multi-Stock Deep Reinforcement Learning Trader")
    print(f"{'='*70}")
    print(f"[Main] Agent Mode: {agent_mode_cfg['mode'].upper()}")
    print(f"[Main] Operation: {operation.upper()}")
    print(f"[Main] Stocks: {len(stock_symbols)}")
    print(f"[Main] Random seed: {seed}")
    print(f"[Main] Max Episodes: {train_cfg['max_episodes']}\n")
    
    # æª¢æŸ¥æ˜¯å¦è¨“ç·´å–®å€‹ Sub-Agent
    if args.train_sub_agent is not None:
        print(f"\n{'='*70}")
        print(f"[Main] ğŸš€ è¨“ç·´å–®å€‹ Sub-Agent")
        print(f"{'='*70}\n")
        
        sub_agents_cfg = agent_mode_cfg.get('sub_agents', [])
        
        if args.train_sub_agent < 0 or args.train_sub_agent >= len(sub_agents_cfg):
            print(f"âŒ ç„¡æ•ˆçš„ Sub-Agent ç´¢å¼•: {args.train_sub_agent}")
            print(f"å¯ç”¨ç´¢å¼•: 0-{len(sub_agents_cfg)-1}")
            return
        
        sub_agent_cfg = sub_agents_cfg[args.train_sub_agent]
        agent_name = sub_agent_cfg.get('name', f'Sub-Agent-{args.train_sub_agent}')
        
        print(f"[Main] Sub-Agent ç´¢å¼•: {args.train_sub_agent}")
        print(f"[Main] åç¨±: {agent_name}")
        print(f"[Main] æ¼”ç®—æ³•: {sub_agent_cfg.get('algorithm', 'a2c').upper()}")
        print(f"[Main] æ¨¡å‹: {sub_agent_cfg.get('model_type', 'mlp').upper()}\n")
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨è¨“ç·´æ—¥æœŸç¯„åœ â˜…â˜…â˜…
        # å‰µå»ºè¨“ç·´ç’°å¢ƒ
        train_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': train_start,
            'end_date': train_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed,
            'agent_type': sub_agent_cfg.get('agent_type', 'direction'),
            'model_type': sub_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        # å‰µå»ºæ¸¬è©¦ç’°å¢ƒ
        test_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': test_start,
            'end_date': test_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed + 1,
            'agent_type': sub_agent_cfg.get('agent_type', 'direction'),
            'model_type': sub_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        # å‰µå»ºè¨“ç·´å™¨ï¼ˆSub-Agent ä¸ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
        trainer = Trainer(
            agent_name=agent_name,
            env=train_env,  # â˜…â˜…â˜… ä½¿ç”¨è¨“ç·´ç’°å¢ƒ
            algorithm=sub_agent_cfg.get('algorithm', 'a2c'),
            max_episodes=train_cfg['max_episodes'],
            update_frequency=train_cfg['update_frequency'],
            model_type=sub_agent_cfg.get('model_type', 'mlp'),
            seed=seed,
            agent_mode='single-agent',
            use_attention=False,  # Sub-Agent ä¸ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶
            actor_lr=actor_lr,
            critic_lr=critic_lr,
            gamma=gamma,
            hidden_dim=hidden_dim,
            batch_size=batch_size,
        )
        
        # æ‰‹å‹•è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        trainer.test_env = test_env  # â˜…â˜…â˜… è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        
        # è¨“ç·´
        print(f"[Main] âœ… é–‹å§‹è¨“ç·´ Sub-Agent: {agent_name}\n")
        trainer.train()
        
        # å„²å­˜æ¨¡å‹
        os.makedirs('./models/sub_agents', exist_ok=True)
        model_path = f"./models/sub_agents/{agent_name}_agent.pth"
        trainer.save_model(model_path)
        print(f"\n[Main] âœ… Sub-Agent è¨“ç·´å®Œæˆï¼")
        print(f"[Main] æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\n")
        
        return
    
    # å¦‚æœåªè¨“ç·´ Final Agent
    if args.train_final_only:
        print(f"\n{'='*70}")
        print(f"[Main] ğŸ¯ è¨“ç·´ Final Agent (ä½¿ç”¨å·²è¨“ç·´çš„ Sub-Agent æ¨¡å‹)")
        print(f"{'='*70}\n")
        
        from trader.parallel_trainer import SubAgentEnsemble
        from trader.envs.final_agent_env import FinalAgentEnv
        
        sub_agents_cfg = agent_mode_cfg.get('sub_agents', [])
        final_agent_cfg = agent_mode_cfg.get('final_agent', {})
        
        # æº–å‚™ Sub-Agent æ¨¡å‹è·¯å¾‘
        model_paths = {}
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨è¨“ç·´æ—¥æœŸå‰µå»ºè‡¨æ™‚ç’°å¢ƒç²å–ç¶­åº¦ â˜…â˜…â˜…
        temp_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': train_start,
            'end_date': train_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed,
            'model_type': final_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        print(f"[Main] æª¢æŸ¥ Sub-Agent æ¨¡å‹...\n")
        all_found = True
        for i, sub_agent in enumerate(sub_agents_cfg):
            agent_name = sub_agent.get('name', f'Sub-Agent-{i}')
            model_path = f"./models/sub_agents/{agent_name}_agent.pth"
            
            if os.path.exists(model_path):
                model_paths[agent_name] = {
                    'path': model_path,
                    'algorithm': sub_agent.get('algorithm', 'a2c'),
                    'model_type': sub_agent.get('model_type', 'mlp'),
                    'state_dim': temp_env.state_dim,
                    'action_dim': temp_env.action_dim,
                    'hidden_dim': hidden_dim,
                }
                size = os.path.getsize(model_path) / 1024 / 1024
                print(f"  âœ“ [{i}] {agent_name}: {size:.2f} MB")
            else:
                print(f"  âœ— [{i}] {agent_name}: NOT FOUND at {model_path}")
                all_found = False
        
        if not all_found:
            print(f"\nâŒ ç¼ºå°‘ä¸€äº› Sub-Agent æ¨¡å‹ï¼Œè«‹å…ˆè¨“ç·´æ‰€æœ‰ Sub-Agents")
            print(f"åŸ·è¡Œä»¥ä¸‹å‘½ä»¤:")
            print(f"  ./run_pipeline.sh train-parallel")
            return
        
        print(f"\nâœ“ æ‰€æœ‰ Sub-Agent æ¨¡å‹å·²æ‰¾åˆ°\n")
        
        # å»ºç«‹ Sub-Agent é›†æˆå™¨
        print(f"[Main] å»ºç«‹ Sub-Agent é›†æˆå™¨...\n")
        ensemble = SubAgentEnsemble(model_paths)
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨è¨“ç·´æ—¥æœŸå‰µå»ºè¨“ç·´ç’°å¢ƒ â˜…â˜…â˜…
        # å‰µå»º Final Agent è¨“ç·´ç’°å¢ƒ
        train_base_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': train_start,
            'end_date': train_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed + 100,
            'agent_type': final_agent_cfg.get('agent_type', 'final'),
            'model_type': final_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        train_final_env = FinalAgentEnv(train_base_env, ensemble)
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨æ¸¬è©¦æ—¥æœŸå‰µå»ºæ¸¬è©¦ç’°å¢ƒ â˜…â˜…â˜…
        # å‰µå»º Final Agent æ¸¬è©¦ç’°å¢ƒ
        test_base_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': test_start,
            'end_date': test_end,
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed + 101,
            'agent_type': final_agent_cfg.get('agent_type', 'final'),
            'model_type': final_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        test_final_env = FinalAgentEnv(test_base_env, ensemble)
        
        # æå– Final Agent æ³¨æ„åŠ›åƒæ•¸
        use_attention = final_agent_cfg.get('use_attention', False)
        num_heads = final_agent_cfg.get('num_heads', 4)
        attention_type = final_agent_cfg.get('attention_type', 'simple')
        
        print(f"[Main] Final Agent é…ç½®:")
        print(f"  - æ¼”ç®—æ³•: {final_agent_cfg.get('algorithm', 'ddpg').upper()}")
        print(f"  - æ¨¡å‹: {final_agent_cfg.get('model_type', 'mlp').upper()}")
        print(f"  - æ³¨æ„åŠ›: {'âœ“ å•Ÿç”¨' if use_attention else 'âœ— ç¦ç”¨'}")
        if use_attention:
            print(f"    - é¡å‹: {attention_type}")
            print(f"    - é ­æ•¸: {num_heads}")
        print()
        
        # å‰µå»º Final Agent Trainer
        final_trainer = Trainer(
            agent_name=final_agent_cfg.get('name', 'Final_Agent'),
            env=train_final_env,  # â˜…â˜…â˜… ä½¿ç”¨è¨“ç·´ç’°å¢ƒ
            algorithm=final_agent_cfg.get('algorithm', 'ddpg'),
            max_episodes=train_cfg['max_episodes'],
            update_frequency=train_cfg['update_frequency'],
            model_type=final_agent_cfg.get('model_type', 'mlp'),
            seed=seed + 100,
            agent_mode='single-agent',
            use_attention=use_attention,
            num_heads=num_heads,
            attention_type=attention_type,
            actor_lr=actor_lr,
            critic_lr=critic_lr,
            gamma=gamma,
            hidden_dim=hidden_dim,
            batch_size=batch_size,
        )
        
        # æ‰‹å‹•è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        final_trainer.test_env = test_final_env  # â˜…â˜…â˜… è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        
        # è¨“ç·´ Final Agent
        print(f"[Main] âœ… é–‹å§‹è¨“ç·´ Final Agent\n")
        final_trainer.train()
        
        # å„²å­˜æ¨¡å‹
        os.makedirs('./models', exist_ok=True)
        final_model_path = f"./models/{final_agent_cfg.get('name', 'Final_Agent')}_agent.pth"
        final_trainer.save_model(final_model_path)
        
        print(f"\n[Main] âœ… Final Agent è¨“ç·´å®Œæˆï¼")
        print(f"[Main] æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}\n")
        
        return

    # ========== Multi-Agent æ¨¡å¼ï¼šä½¿ç”¨ HierarchicalTrainer å¹³è¡Œè¨“ç·´ ==========
    if agent_mode_cfg['mode'] == 'multi-agent':
        sub_agents_cfg = agent_mode_cfg.get('sub_agents', [])
        final_agent_cfg = agent_mode_cfg.get('final_agent', {})
        num_sub_agents = len(sub_agents_cfg)
        
        print(f"[Main] Number of Sub-Agents: {num_sub_agents}")
        print(f"[Main] ğŸš€ ä½¿ç”¨å¹³è¡Œè¨“ç·´æ¨¡å¼ (HierarchicalTrainer)\n")
        
        # é¡¯ç¤ºæ¯å€‹ Sub-Agent çš„é…ç½®
        print(f"[Sub-Agents Configuration]:")
        for i, sub_agent in enumerate(sub_agents_cfg):
            use_attn = sub_agent.get('use_attention', False)
            print(f"  [{i+1}] Name: {sub_agent.get('name', 'Unknown')}")
            print(f"      Algorithm: {sub_agent.get('algorithm', 'N/A').upper()}")
            print(f"      Model Type: {sub_agent.get('model_type', 'N/A').upper()}")
            print(f"      Agent Type: {sub_agent.get('agent_type', 'N/A')}")
            print(f"      Use Attention: {use_attn}")
        
        # é¡¯ç¤º Final Agent çš„é…ç½®
        use_attn = final_agent_cfg.get('use_attention', False)
        attn_type = final_agent_cfg.get('attention_type', 'N/A') if use_attn else 'N/A'
        num_heads = final_agent_cfg.get('num_heads', 'N/A') if use_attn else 'N/A'
        print(f"\n[Final Agent Configuration]:")
        print(f"  Name: {final_agent_cfg.get('name', 'Unknown')}")
        print(f"  Algorithm: {final_agent_cfg.get('algorithm', 'N/A').upper()}")
        print(f"  Model Type: {final_agent_cfg.get('model_type', 'N/A').upper()}")
        print(f"  Agent Type: {final_agent_cfg.get('agent_type', 'N/A')}")
        print(f"  Use Attention: {use_attn}")
        if use_attn:
            print(f"  Attention Type: {attn_type}")
            print(f"  Attention Heads: {num_heads}\n")
        else:
            print()
        
        # å‰µå»º HierarchicalTrainerï¼ˆå¹³è¡Œè¨“ç·´ï¼‰
        hierarchical_trainer = HierarchicalTrainer(config, seed=seed)
        
        if operation == 'training':
            print(f"[Main] âœ… Starting Multi-Agent Parallel Training...\n")
            
            # ç²å– worker æ•¸é‡ï¼ˆå‘½ä»¤åˆ—åƒæ•¸å„ªå…ˆï¼‰
            num_workers = args.num_workers or agent_mode_cfg.get('num_workers', None)
            
            # åŸ·è¡Œå¹³è¡Œè¨“ç·´
            hierarchical_trainer.train(num_workers=num_workers)
            
            # å„²å­˜æ‰€æœ‰æ¨¡å‹
            hierarchical_trainer.save_all_models('./models')
            
            print(f"\n[Main] âœ… Multi-Agent Training Complete!\n")
            
        elif operation == 'evaluation':
            print(f"[Main] âœ… Starting Multi-Agent Evaluation...\n")
            
            # è©•ä¼°
            eval_results = hierarchical_trainer.evaluate(deterministic_seed=True)
            
    
    # ========== Single-Agent æ¨¡å¼ ==========
    else:
        print(f"[Main] Agent Mode: Single-Agent\n")
        
        # å¾ agent_mode é…ç½®è®€å–æ¼”ç®—æ³•
        algorithm = agent_mode_cfg.get('final_agent_algorithm', 'ddpg')
        model_type = agent_mode_cfg.get('final_agent_model_type', 'mlp')
        agent_name = agent_mode_cfg.get('final_agent_name', 'Final_Agent')
        use_attention = agent_mode_cfg.get('use_attention', False)
        num_heads = agent_mode_cfg.get('num_heads', 4)
        attention_type = agent_mode_cfg.get('attention_type', 'simple')
        
        print(f"[Main] Algorithm: {algorithm.upper()}")
        print(f"[Main] Model Type: {model_type.upper()}")
        print(f"[Main] Use Attention: {use_attention}")
        if use_attention:
            print(f"[Main] Attention Type: {attention_type}")
            print(f"[Main] Attention Heads: {num_heads}")
        print()
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨è¨“ç·´æ—¥æœŸå‰µå»ºè¨“ç·´ç’°å¢ƒ â˜…â˜…â˜…
        # å‰µå»ºè¨“ç·´ç’°å¢ƒ
        train_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': train_start,
            'end_date': train_end,
            'k': env_cfg.get('k', 1),
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed,
            'agent_type': sub_agent_cfg.get('agent_type', 'direction'),
            'model_type': sub_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })
        
        # â˜…â˜…â˜… ä¿®æ”¹ï¼šä½¿ç”¨æ¸¬è©¦æ—¥æœŸå‰µå»ºæ¸¬è©¦ç’°å¢ƒ â˜…â˜…â˜…
        # å‰µå»ºæ¸¬è©¦ç’°å¢ƒ
        test_env = EnvironmentFactory.create_trading_env({
            'num_stocks': len(stock_symbols),
            'stock_symbols': stock_symbols,
            'initial_balance': env_cfg['initial_balance'],
            'max_steps': env_cfg['max_steps'],
            'start_date': test_start,
            'end_date': test_end,
            'k': env_cfg.get('k', 1),
            'transaction_cost': env_cfg['transaction_cost'],
            'seed': seed + 1,
            'agent_type': sub_agent_cfg.get('agent_type', 'direction'),
            'model_type': sub_agent_cfg.get('model_type', 'mlp'),
            'window_size': env_cfg.get('window_size', 10)
        })

        # å‰µå»ºè¨“ç·´å™¨
        trainer = Trainer(
            agent_name=agent_name,
            env=train_env,  # â˜…â˜…â˜… ä½¿ç”¨è¨“ç·´ç’°å¢ƒ
            algorithm=algorithm,
            max_episodes=train_cfg['max_episodes'],
            max_timesteps=train_cfg.get('max_timesteps', 50000),
            update_frequency=train_cfg['update_frequency'],
            model_type=model_type,
            seed=seed,
            agent_mode='single-agent',
            use_attention=use_attention,
            num_heads=num_heads,
            attention_type=attention_type,
            actor_lr=actor_lr,
            critic_lr=critic_lr,
            gamma=gamma,
            hidden_dim=hidden_dim,
            batch_size=batch_size,
        )
        
        # æ‰‹å‹•è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        trainer.test_env = test_env  # â˜…â˜…â˜… è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        
        if operation == 'training':
            print(f"[Main] âœ… Starting Single-Agent Training...\n")
            trainer.train()

            os.makedirs('./models', exist_ok=True)
            trainer.save_model(f"./models/{agent_name}.pth")
            print(f"\n[Main] Model saved to ./models/{agent_name}.pth\n")
            
        elif operation == 'evaluation':
            print(f"[Main] âœ… Starting Single-Agent Evaluation...\n")
            
            # è¼‰å…¥æ¨¡å‹
            model_path = args.model or f"./models/{agent_name}.pth"
            if os.path.exists(model_path):
                trainer.load_model(model_path)
                print(f"[Main] Loaded model from {model_path}\n")
            else:
                print(f"[Main] Warning: Model not found at {model_path}, using untrained model\n")
            
            # è©•ä¼°
            eval_results = trainer.evaluate(deterministic_seed=True)

if __name__ == "__main__":
    main()